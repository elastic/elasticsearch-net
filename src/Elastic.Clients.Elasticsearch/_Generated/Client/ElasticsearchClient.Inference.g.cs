// Licensed to Elasticsearch B.V under one or more agreements.
// Elasticsearch B.V licenses this file to you under the Apache 2.0 License.
// See the LICENSE file in the project root for more information.
//
// ███╗   ██╗ ██████╗ ████████╗██╗ ██████╗███████╗
// ████╗  ██║██╔═══██╗╚══██╔══╝██║██╔════╝██╔════╝
// ██╔██╗ ██║██║   ██║   ██║   ██║██║     █████╗
// ██║╚██╗██║██║   ██║   ██║   ██║██║     ██╔══╝
// ██║ ╚████║╚██████╔╝   ██║   ██║╚██████╗███████╗
// ╚═╝  ╚═══╝ ╚═════╝    ╚═╝   ╚═╝ ╚═════╝╚══════╝
// ------------------------------------------------
//
// This file is automatically generated.
// Please do not edit these files manually.
//
// ------------------------------------------------

#nullable restore

using System;
using System.Collections.Generic;
using System.Threading;
using System.Threading.Tasks;

namespace Elastic.Clients.Elasticsearch.Inference;

public partial class InferenceNamespacedClient : NamespacedClientProxy
{
	/// <summary>
	/// <para>
	/// Initializes a new instance of the <see cref="InferenceNamespacedClient"/> class for mocking.
	/// </para>
	/// </summary>
	protected InferenceNamespacedClient() : base()
	{
	}

	internal InferenceNamespacedClient(ElasticsearchClient client) : base(client)
	{
	}

	/// <summary>
	/// <para>
	/// Delete an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual DeleteInferenceResponse Delete(DeleteInferenceRequest request)
	{
		request.BeforeRequest();
		return DoRequest<DeleteInferenceRequest, DeleteInferenceResponse, DeleteInferenceRequestParameters>(request);
	}

	/// <summary>
	/// <para>
	/// Delete an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<DeleteInferenceResponse> DeleteAsync(DeleteInferenceRequest request, CancellationToken cancellationToken = default)
	{
		request.BeforeRequest();
		return DoRequestAsync<DeleteInferenceRequest, DeleteInferenceResponse, DeleteInferenceRequestParameters>(request, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Delete an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual DeleteInferenceResponse Delete(DeleteInferenceRequestDescriptor descriptor)
	{
		descriptor.BeforeRequest();
		return DoRequest<DeleteInferenceRequestDescriptor, DeleteInferenceResponse, DeleteInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Delete an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual DeleteInferenceResponse Delete(Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId)
	{
		var descriptor = new DeleteInferenceRequestDescriptor(taskType, inferenceId);
		descriptor.BeforeRequest();
		return DoRequest<DeleteInferenceRequestDescriptor, DeleteInferenceResponse, DeleteInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Delete an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual DeleteInferenceResponse Delete(Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId, Action<DeleteInferenceRequestDescriptor> configureRequest)
	{
		var descriptor = new DeleteInferenceRequestDescriptor(taskType, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequest<DeleteInferenceRequestDescriptor, DeleteInferenceResponse, DeleteInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Delete an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual DeleteInferenceResponse Delete(Elastic.Clients.Elasticsearch.Id inferenceId)
	{
		var descriptor = new DeleteInferenceRequestDescriptor(inferenceId);
		descriptor.BeforeRequest();
		return DoRequest<DeleteInferenceRequestDescriptor, DeleteInferenceResponse, DeleteInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Delete an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual DeleteInferenceResponse Delete(Elastic.Clients.Elasticsearch.Id inferenceId, Action<DeleteInferenceRequestDescriptor> configureRequest)
	{
		var descriptor = new DeleteInferenceRequestDescriptor(inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequest<DeleteInferenceRequestDescriptor, DeleteInferenceResponse, DeleteInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Delete an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<DeleteInferenceResponse> DeleteAsync(DeleteInferenceRequestDescriptor descriptor, CancellationToken cancellationToken = default)
	{
		descriptor.BeforeRequest();
		return DoRequestAsync<DeleteInferenceRequestDescriptor, DeleteInferenceResponse, DeleteInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Delete an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<DeleteInferenceResponse> DeleteAsync(Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId, CancellationToken cancellationToken = default)
	{
		var descriptor = new DeleteInferenceRequestDescriptor(taskType, inferenceId);
		descriptor.BeforeRequest();
		return DoRequestAsync<DeleteInferenceRequestDescriptor, DeleteInferenceResponse, DeleteInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Delete an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<DeleteInferenceResponse> DeleteAsync(Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId, Action<DeleteInferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new DeleteInferenceRequestDescriptor(taskType, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<DeleteInferenceRequestDescriptor, DeleteInferenceResponse, DeleteInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Delete an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<DeleteInferenceResponse> DeleteAsync(Elastic.Clients.Elasticsearch.Id inferenceId, CancellationToken cancellationToken = default)
	{
		var descriptor = new DeleteInferenceRequestDescriptor(inferenceId);
		descriptor.BeforeRequest();
		return DoRequestAsync<DeleteInferenceRequestDescriptor, DeleteInferenceResponse, DeleteInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Delete an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<DeleteInferenceResponse> DeleteAsync(Elastic.Clients.Elasticsearch.Id inferenceId, Action<DeleteInferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new DeleteInferenceRequestDescriptor(inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<DeleteInferenceRequestDescriptor, DeleteInferenceResponse, DeleteInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Get an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/get-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual GetInferenceResponse Get(GetInferenceRequest request)
	{
		request.BeforeRequest();
		return DoRequest<GetInferenceRequest, GetInferenceResponse, GetInferenceRequestParameters>(request);
	}

	/// <summary>
	/// <para>
	/// Get an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/get-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<GetInferenceResponse> GetAsync(GetInferenceRequest request, CancellationToken cancellationToken = default)
	{
		request.BeforeRequest();
		return DoRequestAsync<GetInferenceRequest, GetInferenceResponse, GetInferenceRequestParameters>(request, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Get an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/get-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual GetInferenceResponse Get(GetInferenceRequestDescriptor descriptor)
	{
		descriptor.BeforeRequest();
		return DoRequest<GetInferenceRequestDescriptor, GetInferenceResponse, GetInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Get an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/get-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual GetInferenceResponse Get(Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id? inferenceId)
	{
		var descriptor = new GetInferenceRequestDescriptor(taskType, inferenceId);
		descriptor.BeforeRequest();
		return DoRequest<GetInferenceRequestDescriptor, GetInferenceResponse, GetInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Get an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/get-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual GetInferenceResponse Get(Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id? inferenceId, Action<GetInferenceRequestDescriptor> configureRequest)
	{
		var descriptor = new GetInferenceRequestDescriptor(taskType, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequest<GetInferenceRequestDescriptor, GetInferenceResponse, GetInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Get an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/get-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual GetInferenceResponse Get()
	{
		var descriptor = new GetInferenceRequestDescriptor();
		descriptor.BeforeRequest();
		return DoRequest<GetInferenceRequestDescriptor, GetInferenceResponse, GetInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Get an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/get-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual GetInferenceResponse Get(Action<GetInferenceRequestDescriptor> configureRequest)
	{
		var descriptor = new GetInferenceRequestDescriptor();
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequest<GetInferenceRequestDescriptor, GetInferenceResponse, GetInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Get an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/get-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<GetInferenceResponse> GetAsync(GetInferenceRequestDescriptor descriptor, CancellationToken cancellationToken = default)
	{
		descriptor.BeforeRequest();
		return DoRequestAsync<GetInferenceRequestDescriptor, GetInferenceResponse, GetInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Get an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/get-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<GetInferenceResponse> GetAsync(Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id? inferenceId, CancellationToken cancellationToken = default)
	{
		var descriptor = new GetInferenceRequestDescriptor(taskType, inferenceId);
		descriptor.BeforeRequest();
		return DoRequestAsync<GetInferenceRequestDescriptor, GetInferenceResponse, GetInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Get an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/get-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<GetInferenceResponse> GetAsync(Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id? inferenceId, Action<GetInferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new GetInferenceRequestDescriptor(taskType, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<GetInferenceRequestDescriptor, GetInferenceResponse, GetInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Get an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/get-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<GetInferenceResponse> GetAsync(CancellationToken cancellationToken = default)
	{
		var descriptor = new GetInferenceRequestDescriptor();
		descriptor.BeforeRequest();
		return DoRequestAsync<GetInferenceRequestDescriptor, GetInferenceResponse, GetInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Get an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/get-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<GetInferenceResponse> GetAsync(Action<GetInferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new GetInferenceRequestDescriptor();
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<GetInferenceRequestDescriptor, GetInferenceResponse, GetInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Perform inference on the service
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/post-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual InferenceResponse Inference(InferenceRequest request)
	{
		request.BeforeRequest();
		return DoRequest<InferenceRequest, InferenceResponse, InferenceRequestParameters>(request);
	}

	/// <summary>
	/// <para>
	/// Perform inference on the service
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/post-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<InferenceResponse> InferenceAsync(InferenceRequest request, CancellationToken cancellationToken = default)
	{
		request.BeforeRequest();
		return DoRequestAsync<InferenceRequest, InferenceResponse, InferenceRequestParameters>(request, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Perform inference on the service
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/post-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual InferenceResponse Inference(InferenceRequestDescriptor descriptor)
	{
		descriptor.BeforeRequest();
		return DoRequest<InferenceRequestDescriptor, InferenceResponse, InferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Perform inference on the service
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/post-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual InferenceResponse Inference(Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId)
	{
		var descriptor = new InferenceRequestDescriptor(taskType, inferenceId);
		descriptor.BeforeRequest();
		return DoRequest<InferenceRequestDescriptor, InferenceResponse, InferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Perform inference on the service
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/post-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual InferenceResponse Inference(Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId, Action<InferenceRequestDescriptor> configureRequest)
	{
		var descriptor = new InferenceRequestDescriptor(taskType, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequest<InferenceRequestDescriptor, InferenceResponse, InferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Perform inference on the service
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/post-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual InferenceResponse Inference(Elastic.Clients.Elasticsearch.Id inferenceId)
	{
		var descriptor = new InferenceRequestDescriptor(inferenceId);
		descriptor.BeforeRequest();
		return DoRequest<InferenceRequestDescriptor, InferenceResponse, InferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Perform inference on the service
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/post-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual InferenceResponse Inference(Elastic.Clients.Elasticsearch.Id inferenceId, Action<InferenceRequestDescriptor> configureRequest)
	{
		var descriptor = new InferenceRequestDescriptor(inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequest<InferenceRequestDescriptor, InferenceResponse, InferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Perform inference on the service
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/post-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<InferenceResponse> InferenceAsync(InferenceRequestDescriptor descriptor, CancellationToken cancellationToken = default)
	{
		descriptor.BeforeRequest();
		return DoRequestAsync<InferenceRequestDescriptor, InferenceResponse, InferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Perform inference on the service
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/post-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<InferenceResponse> InferenceAsync(Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId, CancellationToken cancellationToken = default)
	{
		var descriptor = new InferenceRequestDescriptor(taskType, inferenceId);
		descriptor.BeforeRequest();
		return DoRequestAsync<InferenceRequestDescriptor, InferenceResponse, InferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Perform inference on the service
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/post-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<InferenceResponse> InferenceAsync(Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId, Action<InferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new InferenceRequestDescriptor(taskType, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<InferenceRequestDescriptor, InferenceResponse, InferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Perform inference on the service
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/post-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<InferenceResponse> InferenceAsync(Elastic.Clients.Elasticsearch.Id inferenceId, CancellationToken cancellationToken = default)
	{
		var descriptor = new InferenceRequestDescriptor(inferenceId);
		descriptor.BeforeRequest();
		return DoRequestAsync<InferenceRequestDescriptor, InferenceResponse, InferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Perform inference on the service
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/post-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<InferenceResponse> InferenceAsync(Elastic.Clients.Elasticsearch.Id inferenceId, Action<InferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new InferenceRequestDescriptor(inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<InferenceRequestDescriptor, InferenceResponse, InferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Create an inference endpoint.
	/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
	/// After creating the endpoint, wait for the model deployment to complete before using it.
	/// To verify the deployment status, use the get trained model statistics API.
	/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
	/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Mistral, Azure OpenAI, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual PutInferenceResponse Put(PutInferenceRequest request)
	{
		request.BeforeRequest();
		return DoRequest<PutInferenceRequest, PutInferenceResponse, PutInferenceRequestParameters>(request);
	}

	/// <summary>
	/// <para>
	/// Create an inference endpoint.
	/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
	/// After creating the endpoint, wait for the model deployment to complete before using it.
	/// To verify the deployment status, use the get trained model statistics API.
	/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
	/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Mistral, Azure OpenAI, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<PutInferenceResponse> PutAsync(PutInferenceRequest request, CancellationToken cancellationToken = default)
	{
		request.BeforeRequest();
		return DoRequestAsync<PutInferenceRequest, PutInferenceResponse, PutInferenceRequestParameters>(request, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Create an inference endpoint.
	/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
	/// After creating the endpoint, wait for the model deployment to complete before using it.
	/// To verify the deployment status, use the get trained model statistics API.
	/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
	/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Mistral, Azure OpenAI, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual PutInferenceResponse Put(PutInferenceRequestDescriptor descriptor)
	{
		descriptor.BeforeRequest();
		return DoRequest<PutInferenceRequestDescriptor, PutInferenceResponse, PutInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Create an inference endpoint.
	/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
	/// After creating the endpoint, wait for the model deployment to complete before using it.
	/// To verify the deployment status, use the get trained model statistics API.
	/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
	/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Mistral, Azure OpenAI, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual PutInferenceResponse Put(Elastic.Clients.Elasticsearch.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId)
	{
		var descriptor = new PutInferenceRequestDescriptor(inferenceConfig, taskType, inferenceId);
		descriptor.BeforeRequest();
		return DoRequest<PutInferenceRequestDescriptor, PutInferenceResponse, PutInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Create an inference endpoint.
	/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
	/// After creating the endpoint, wait for the model deployment to complete before using it.
	/// To verify the deployment status, use the get trained model statistics API.
	/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
	/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Mistral, Azure OpenAI, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual PutInferenceResponse Put(Elastic.Clients.Elasticsearch.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId, Action<PutInferenceRequestDescriptor> configureRequest)
	{
		var descriptor = new PutInferenceRequestDescriptor(inferenceConfig, taskType, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequest<PutInferenceRequestDescriptor, PutInferenceResponse, PutInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Create an inference endpoint.
	/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
	/// After creating the endpoint, wait for the model deployment to complete before using it.
	/// To verify the deployment status, use the get trained model statistics API.
	/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
	/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Mistral, Azure OpenAI, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual PutInferenceResponse Put(Elastic.Clients.Elasticsearch.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Id inferenceId)
	{
		var descriptor = new PutInferenceRequestDescriptor(inferenceConfig, inferenceId);
		descriptor.BeforeRequest();
		return DoRequest<PutInferenceRequestDescriptor, PutInferenceResponse, PutInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Create an inference endpoint.
	/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
	/// After creating the endpoint, wait for the model deployment to complete before using it.
	/// To verify the deployment status, use the get trained model statistics API.
	/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
	/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Mistral, Azure OpenAI, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual PutInferenceResponse Put(Elastic.Clients.Elasticsearch.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Id inferenceId, Action<PutInferenceRequestDescriptor> configureRequest)
	{
		var descriptor = new PutInferenceRequestDescriptor(inferenceConfig, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequest<PutInferenceRequestDescriptor, PutInferenceResponse, PutInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Create an inference endpoint.
	/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
	/// After creating the endpoint, wait for the model deployment to complete before using it.
	/// To verify the deployment status, use the get trained model statistics API.
	/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
	/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Mistral, Azure OpenAI, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<PutInferenceResponse> PutAsync(PutInferenceRequestDescriptor descriptor, CancellationToken cancellationToken = default)
	{
		descriptor.BeforeRequest();
		return DoRequestAsync<PutInferenceRequestDescriptor, PutInferenceResponse, PutInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Create an inference endpoint.
	/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
	/// After creating the endpoint, wait for the model deployment to complete before using it.
	/// To verify the deployment status, use the get trained model statistics API.
	/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
	/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Mistral, Azure OpenAI, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<PutInferenceResponse> PutAsync(Elastic.Clients.Elasticsearch.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId, CancellationToken cancellationToken = default)
	{
		var descriptor = new PutInferenceRequestDescriptor(inferenceConfig, taskType, inferenceId);
		descriptor.BeforeRequest();
		return DoRequestAsync<PutInferenceRequestDescriptor, PutInferenceResponse, PutInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Create an inference endpoint.
	/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
	/// After creating the endpoint, wait for the model deployment to complete before using it.
	/// To verify the deployment status, use the get trained model statistics API.
	/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
	/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Mistral, Azure OpenAI, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<PutInferenceResponse> PutAsync(Elastic.Clients.Elasticsearch.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId, Action<PutInferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new PutInferenceRequestDescriptor(inferenceConfig, taskType, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<PutInferenceRequestDescriptor, PutInferenceResponse, PutInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Create an inference endpoint.
	/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
	/// After creating the endpoint, wait for the model deployment to complete before using it.
	/// To verify the deployment status, use the get trained model statistics API.
	/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
	/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Mistral, Azure OpenAI, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<PutInferenceResponse> PutAsync(Elastic.Clients.Elasticsearch.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Id inferenceId, CancellationToken cancellationToken = default)
	{
		var descriptor = new PutInferenceRequestDescriptor(inferenceConfig, inferenceId);
		descriptor.BeforeRequest();
		return DoRequestAsync<PutInferenceRequestDescriptor, PutInferenceResponse, PutInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Create an inference endpoint.
	/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
	/// After creating the endpoint, wait for the model deployment to complete before using it.
	/// To verify the deployment status, use the get trained model statistics API.
	/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
	/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Mistral, Azure OpenAI, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<PutInferenceResponse> PutAsync(Elastic.Clients.Elasticsearch.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Id inferenceId, Action<PutInferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new PutInferenceRequestDescriptor(inferenceConfig, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<PutInferenceRequestDescriptor, PutInferenceResponse, PutInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Perform streaming inference.
	/// Get real-time responses for completion tasks by delivering answers incrementally, reducing response times during computation.
	/// This API works only with the completion task type.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face. For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models. However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para>
	/// This API requires the <c>monitor_inference</c> cluster privilege (the built-in <c>inference_admin</c> and <c>inference_user</c> roles grant this privilege). You must use a client that supports streaming.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/stream-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual StreamInferenceResponse StreamInference(StreamInferenceRequest request)
	{
		request.BeforeRequest();
		return DoRequest<StreamInferenceRequest, StreamInferenceResponse, StreamInferenceRequestParameters>(request);
	}

	/// <summary>
	/// <para>
	/// Perform streaming inference.
	/// Get real-time responses for completion tasks by delivering answers incrementally, reducing response times during computation.
	/// This API works only with the completion task type.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face. For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models. However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para>
	/// This API requires the <c>monitor_inference</c> cluster privilege (the built-in <c>inference_admin</c> and <c>inference_user</c> roles grant this privilege). You must use a client that supports streaming.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/stream-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<StreamInferenceResponse> StreamInferenceAsync(StreamInferenceRequest request, CancellationToken cancellationToken = default)
	{
		request.BeforeRequest();
		return DoRequestAsync<StreamInferenceRequest, StreamInferenceResponse, StreamInferenceRequestParameters>(request, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Perform streaming inference.
	/// Get real-time responses for completion tasks by delivering answers incrementally, reducing response times during computation.
	/// This API works only with the completion task type.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face. For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models. However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para>
	/// This API requires the <c>monitor_inference</c> cluster privilege (the built-in <c>inference_admin</c> and <c>inference_user</c> roles grant this privilege). You must use a client that supports streaming.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/stream-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual StreamInferenceResponse StreamInference(StreamInferenceRequestDescriptor descriptor)
	{
		descriptor.BeforeRequest();
		return DoRequest<StreamInferenceRequestDescriptor, StreamInferenceResponse, StreamInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Perform streaming inference.
	/// Get real-time responses for completion tasks by delivering answers incrementally, reducing response times during computation.
	/// This API works only with the completion task type.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face. For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models. However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para>
	/// This API requires the <c>monitor_inference</c> cluster privilege (the built-in <c>inference_admin</c> and <c>inference_user</c> roles grant this privilege). You must use a client that supports streaming.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/stream-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual StreamInferenceResponse StreamInference(Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId)
	{
		var descriptor = new StreamInferenceRequestDescriptor(taskType, inferenceId);
		descriptor.BeforeRequest();
		return DoRequest<StreamInferenceRequestDescriptor, StreamInferenceResponse, StreamInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Perform streaming inference.
	/// Get real-time responses for completion tasks by delivering answers incrementally, reducing response times during computation.
	/// This API works only with the completion task type.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face. For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models. However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para>
	/// This API requires the <c>monitor_inference</c> cluster privilege (the built-in <c>inference_admin</c> and <c>inference_user</c> roles grant this privilege). You must use a client that supports streaming.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/stream-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual StreamInferenceResponse StreamInference(Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId, Action<StreamInferenceRequestDescriptor> configureRequest)
	{
		var descriptor = new StreamInferenceRequestDescriptor(taskType, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequest<StreamInferenceRequestDescriptor, StreamInferenceResponse, StreamInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Perform streaming inference.
	/// Get real-time responses for completion tasks by delivering answers incrementally, reducing response times during computation.
	/// This API works only with the completion task type.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face. For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models. However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para>
	/// This API requires the <c>monitor_inference</c> cluster privilege (the built-in <c>inference_admin</c> and <c>inference_user</c> roles grant this privilege). You must use a client that supports streaming.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/stream-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual StreamInferenceResponse StreamInference(Elastic.Clients.Elasticsearch.Id inferenceId)
	{
		var descriptor = new StreamInferenceRequestDescriptor(inferenceId);
		descriptor.BeforeRequest();
		return DoRequest<StreamInferenceRequestDescriptor, StreamInferenceResponse, StreamInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Perform streaming inference.
	/// Get real-time responses for completion tasks by delivering answers incrementally, reducing response times during computation.
	/// This API works only with the completion task type.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face. For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models. However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para>
	/// This API requires the <c>monitor_inference</c> cluster privilege (the built-in <c>inference_admin</c> and <c>inference_user</c> roles grant this privilege). You must use a client that supports streaming.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/stream-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual StreamInferenceResponse StreamInference(Elastic.Clients.Elasticsearch.Id inferenceId, Action<StreamInferenceRequestDescriptor> configureRequest)
	{
		var descriptor = new StreamInferenceRequestDescriptor(inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequest<StreamInferenceRequestDescriptor, StreamInferenceResponse, StreamInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Perform streaming inference.
	/// Get real-time responses for completion tasks by delivering answers incrementally, reducing response times during computation.
	/// This API works only with the completion task type.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face. For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models. However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para>
	/// This API requires the <c>monitor_inference</c> cluster privilege (the built-in <c>inference_admin</c> and <c>inference_user</c> roles grant this privilege). You must use a client that supports streaming.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/stream-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<StreamInferenceResponse> StreamInferenceAsync(StreamInferenceRequestDescriptor descriptor, CancellationToken cancellationToken = default)
	{
		descriptor.BeforeRequest();
		return DoRequestAsync<StreamInferenceRequestDescriptor, StreamInferenceResponse, StreamInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Perform streaming inference.
	/// Get real-time responses for completion tasks by delivering answers incrementally, reducing response times during computation.
	/// This API works only with the completion task type.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face. For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models. However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para>
	/// This API requires the <c>monitor_inference</c> cluster privilege (the built-in <c>inference_admin</c> and <c>inference_user</c> roles grant this privilege). You must use a client that supports streaming.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/stream-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<StreamInferenceResponse> StreamInferenceAsync(Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId, CancellationToken cancellationToken = default)
	{
		var descriptor = new StreamInferenceRequestDescriptor(taskType, inferenceId);
		descriptor.BeforeRequest();
		return DoRequestAsync<StreamInferenceRequestDescriptor, StreamInferenceResponse, StreamInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Perform streaming inference.
	/// Get real-time responses for completion tasks by delivering answers incrementally, reducing response times during computation.
	/// This API works only with the completion task type.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face. For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models. However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para>
	/// This API requires the <c>monitor_inference</c> cluster privilege (the built-in <c>inference_admin</c> and <c>inference_user</c> roles grant this privilege). You must use a client that supports streaming.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/stream-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<StreamInferenceResponse> StreamInferenceAsync(Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId, Action<StreamInferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new StreamInferenceRequestDescriptor(taskType, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<StreamInferenceRequestDescriptor, StreamInferenceResponse, StreamInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Perform streaming inference.
	/// Get real-time responses for completion tasks by delivering answers incrementally, reducing response times during computation.
	/// This API works only with the completion task type.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face. For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models. However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para>
	/// This API requires the <c>monitor_inference</c> cluster privilege (the built-in <c>inference_admin</c> and <c>inference_user</c> roles grant this privilege). You must use a client that supports streaming.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/stream-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<StreamInferenceResponse> StreamInferenceAsync(Elastic.Clients.Elasticsearch.Id inferenceId, CancellationToken cancellationToken = default)
	{
		var descriptor = new StreamInferenceRequestDescriptor(inferenceId);
		descriptor.BeforeRequest();
		return DoRequestAsync<StreamInferenceRequestDescriptor, StreamInferenceResponse, StreamInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Perform streaming inference.
	/// Get real-time responses for completion tasks by delivering answers incrementally, reducing response times during computation.
	/// This API works only with the completion task type.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face. For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models. However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para>
	/// This API requires the <c>monitor_inference</c> cluster privilege (the built-in <c>inference_admin</c> and <c>inference_user</c> roles grant this privilege). You must use a client that supports streaming.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/stream-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<StreamInferenceResponse> StreamInferenceAsync(Elastic.Clients.Elasticsearch.Id inferenceId, Action<StreamInferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new StreamInferenceRequestDescriptor(inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<StreamInferenceRequestDescriptor, StreamInferenceResponse, StreamInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Update an inference endpoint.
	/// </para>
	/// <para>
	/// Modify <c>task_settings</c>, secrets (within <c>service_settings</c>), or <c>num_allocations</c> for an inference endpoint, depending on the specific endpoint service and <c>task_type</c>.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/update-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual UpdateInferenceResponse Update(UpdateInferenceRequest request)
	{
		request.BeforeRequest();
		return DoRequest<UpdateInferenceRequest, UpdateInferenceResponse, UpdateInferenceRequestParameters>(request);
	}

	/// <summary>
	/// <para>
	/// Update an inference endpoint.
	/// </para>
	/// <para>
	/// Modify <c>task_settings</c>, secrets (within <c>service_settings</c>), or <c>num_allocations</c> for an inference endpoint, depending on the specific endpoint service and <c>task_type</c>.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/update-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<UpdateInferenceResponse> UpdateAsync(UpdateInferenceRequest request, CancellationToken cancellationToken = default)
	{
		request.BeforeRequest();
		return DoRequestAsync<UpdateInferenceRequest, UpdateInferenceResponse, UpdateInferenceRequestParameters>(request, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Update an inference endpoint.
	/// </para>
	/// <para>
	/// Modify <c>task_settings</c>, secrets (within <c>service_settings</c>), or <c>num_allocations</c> for an inference endpoint, depending on the specific endpoint service and <c>task_type</c>.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/update-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual UpdateInferenceResponse Update(UpdateInferenceRequestDescriptor descriptor)
	{
		descriptor.BeforeRequest();
		return DoRequest<UpdateInferenceRequestDescriptor, UpdateInferenceResponse, UpdateInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Update an inference endpoint.
	/// </para>
	/// <para>
	/// Modify <c>task_settings</c>, secrets (within <c>service_settings</c>), or <c>num_allocations</c> for an inference endpoint, depending on the specific endpoint service and <c>task_type</c>.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/update-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual UpdateInferenceResponse Update(Elastic.Clients.Elasticsearch.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId)
	{
		var descriptor = new UpdateInferenceRequestDescriptor(inferenceConfig, taskType, inferenceId);
		descriptor.BeforeRequest();
		return DoRequest<UpdateInferenceRequestDescriptor, UpdateInferenceResponse, UpdateInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Update an inference endpoint.
	/// </para>
	/// <para>
	/// Modify <c>task_settings</c>, secrets (within <c>service_settings</c>), or <c>num_allocations</c> for an inference endpoint, depending on the specific endpoint service and <c>task_type</c>.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/update-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual UpdateInferenceResponse Update(Elastic.Clients.Elasticsearch.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId, Action<UpdateInferenceRequestDescriptor> configureRequest)
	{
		var descriptor = new UpdateInferenceRequestDescriptor(inferenceConfig, taskType, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequest<UpdateInferenceRequestDescriptor, UpdateInferenceResponse, UpdateInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Update an inference endpoint.
	/// </para>
	/// <para>
	/// Modify <c>task_settings</c>, secrets (within <c>service_settings</c>), or <c>num_allocations</c> for an inference endpoint, depending on the specific endpoint service and <c>task_type</c>.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/update-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual UpdateInferenceResponse Update(Elastic.Clients.Elasticsearch.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Id inferenceId)
	{
		var descriptor = new UpdateInferenceRequestDescriptor(inferenceConfig, inferenceId);
		descriptor.BeforeRequest();
		return DoRequest<UpdateInferenceRequestDescriptor, UpdateInferenceResponse, UpdateInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Update an inference endpoint.
	/// </para>
	/// <para>
	/// Modify <c>task_settings</c>, secrets (within <c>service_settings</c>), or <c>num_allocations</c> for an inference endpoint, depending on the specific endpoint service and <c>task_type</c>.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/update-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	[Obsolete("Synchronous methods are deprecated and could be removed in the future.")]
	public virtual UpdateInferenceResponse Update(Elastic.Clients.Elasticsearch.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Id inferenceId, Action<UpdateInferenceRequestDescriptor> configureRequest)
	{
		var descriptor = new UpdateInferenceRequestDescriptor(inferenceConfig, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequest<UpdateInferenceRequestDescriptor, UpdateInferenceResponse, UpdateInferenceRequestParameters>(descriptor);
	}

	/// <summary>
	/// <para>
	/// Update an inference endpoint.
	/// </para>
	/// <para>
	/// Modify <c>task_settings</c>, secrets (within <c>service_settings</c>), or <c>num_allocations</c> for an inference endpoint, depending on the specific endpoint service and <c>task_type</c>.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/update-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<UpdateInferenceResponse> UpdateAsync(UpdateInferenceRequestDescriptor descriptor, CancellationToken cancellationToken = default)
	{
		descriptor.BeforeRequest();
		return DoRequestAsync<UpdateInferenceRequestDescriptor, UpdateInferenceResponse, UpdateInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Update an inference endpoint.
	/// </para>
	/// <para>
	/// Modify <c>task_settings</c>, secrets (within <c>service_settings</c>), or <c>num_allocations</c> for an inference endpoint, depending on the specific endpoint service and <c>task_type</c>.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/update-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<UpdateInferenceResponse> UpdateAsync(Elastic.Clients.Elasticsearch.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId, CancellationToken cancellationToken = default)
	{
		var descriptor = new UpdateInferenceRequestDescriptor(inferenceConfig, taskType, inferenceId);
		descriptor.BeforeRequest();
		return DoRequestAsync<UpdateInferenceRequestDescriptor, UpdateInferenceResponse, UpdateInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Update an inference endpoint.
	/// </para>
	/// <para>
	/// Modify <c>task_settings</c>, secrets (within <c>service_settings</c>), or <c>num_allocations</c> for an inference endpoint, depending on the specific endpoint service and <c>task_type</c>.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/update-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<UpdateInferenceResponse> UpdateAsync(Elastic.Clients.Elasticsearch.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Id inferenceId, Action<UpdateInferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new UpdateInferenceRequestDescriptor(inferenceConfig, taskType, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<UpdateInferenceRequestDescriptor, UpdateInferenceResponse, UpdateInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Update an inference endpoint.
	/// </para>
	/// <para>
	/// Modify <c>task_settings</c>, secrets (within <c>service_settings</c>), or <c>num_allocations</c> for an inference endpoint, depending on the specific endpoint service and <c>task_type</c>.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/update-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<UpdateInferenceResponse> UpdateAsync(Elastic.Clients.Elasticsearch.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Id inferenceId, CancellationToken cancellationToken = default)
	{
		var descriptor = new UpdateInferenceRequestDescriptor(inferenceConfig, inferenceId);
		descriptor.BeforeRequest();
		return DoRequestAsync<UpdateInferenceRequestDescriptor, UpdateInferenceResponse, UpdateInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Update an inference endpoint.
	/// </para>
	/// <para>
	/// Modify <c>task_settings</c>, secrets (within <c>service_settings</c>), or <c>num_allocations</c> for an inference endpoint, depending on the specific endpoint service and <c>task_type</c>.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/8.17/update-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<UpdateInferenceResponse> UpdateAsync(Elastic.Clients.Elasticsearch.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Id inferenceId, Action<UpdateInferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new UpdateInferenceRequestDescriptor(inferenceConfig, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<UpdateInferenceRequestDescriptor, UpdateInferenceResponse, UpdateInferenceRequestParameters>(descriptor, cancellationToken);
	}
}