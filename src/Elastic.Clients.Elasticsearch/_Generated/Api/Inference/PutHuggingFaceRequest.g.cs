// Licensed to Elasticsearch B.V under one or more agreements.
// Elasticsearch B.V licenses this file to you under the Apache 2.0 License.
// See the LICENSE file in the project root for more information.
//
// ███╗   ██╗ ██████╗ ████████╗██╗ ██████╗███████╗
// ████╗  ██║██╔═══██╗╚══██╔══╝██║██╔════╝██╔════╝
// ██╔██╗ ██║██║   ██║   ██║   ██║██║     █████╗
// ██║╚██╗██║██║   ██║   ██║   ██║██║     ██╔══╝
// ██║ ╚████║╚██████╔╝   ██║   ██║╚██████╗███████╗
// ╚═╝  ╚═══╝ ╚═════╝    ╚═╝   ╚═╝ ╚═════╝╚══════╝
// ------------------------------------------------
//
// This file is automatically generated.
// Please do not edit these files manually.
//
// ------------------------------------------------

#nullable restore

using Elastic.Clients.Elasticsearch.Fluent;
using Elastic.Clients.Elasticsearch.Requests;
using Elastic.Clients.Elasticsearch.Serialization;
using Elastic.Transport;
using Elastic.Transport.Extensions;
using System;
using System.Collections.Generic;
using System.Linq.Expressions;
using System.Text.Json;
using System.Text.Json.Serialization;

namespace Elastic.Clients.Elasticsearch.Inference;

public sealed partial class PutHuggingFaceRequestParameters : RequestParameters
{
}

/// <summary>
/// <para>
/// Create a Hugging Face inference endpoint.
/// </para>
/// <para>
/// Create an inference endpoint to perform an inference task with the <c>hugging_face</c> service.
/// </para>
/// <para>
/// You must first create an inference endpoint on the Hugging Face endpoint page to get an endpoint URL.
/// Select the model you want to use on the new endpoint creation page (for example <c>intfloat/e5-small-v2</c>), then select the sentence embeddings task under the advanced configuration section.
/// Create the endpoint and copy the URL after the endpoint initialization has been finished.
/// </para>
/// <para>
/// The following models are recommended for the Hugging Face service:
/// </para>
/// <list type="bullet">
/// <item>
/// <para>
/// <c>all-MiniLM-L6-v2</c>
/// </para>
/// </item>
/// <item>
/// <para>
/// <c>all-MiniLM-L12-v2</c>
/// </para>
/// </item>
/// <item>
/// <para>
/// <c>all-mpnet-base-v2</c>
/// </para>
/// </item>
/// <item>
/// <para>
/// <c>e5-base-v2</c>
/// </para>
/// </item>
/// <item>
/// <para>
/// <c>e5-small-v2</c>
/// </para>
/// </item>
/// <item>
/// <para>
/// <c>multilingual-e5-base</c>
/// </para>
/// </item>
/// <item>
/// <para>
/// <c>multilingual-e5-small</c>
/// </para>
/// </item>
/// </list>
/// <para>
/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
/// After creating the endpoint, wait for the model deployment to complete before using it.
/// To verify the deployment status, use the get trained model statistics API.
/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
/// </para>
/// </summary>
public sealed partial class PutHuggingFaceRequest : PlainRequest<PutHuggingFaceRequestParameters>
{
	public PutHuggingFaceRequest(Elastic.Clients.Elasticsearch.Inference.HuggingFaceTaskType taskType, Elastic.Clients.Elasticsearch.Id huggingfaceInferenceId) : base(r => r.Required("task_type", taskType).Required("huggingface_inference_id", huggingfaceInferenceId))
	{
	}

	internal override ApiUrls ApiUrls => ApiUrlLookup.InferencePutHuggingFace;

	protected override HttpMethod StaticHttpMethod => HttpMethod.PUT;

	internal override bool SupportsBody => true;

	internal override string OperationName => "inference.put_hugging_face";

	/// <summary>
	/// <para>
	/// The chunking configuration object.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("chunking_settings")]
	public Elastic.Clients.Elasticsearch.Inference.InferenceChunkingSettings? ChunkingSettings { get; set; }

	/// <summary>
	/// <para>
	/// The type of service supported for the specified task type. In this case, <c>hugging_face</c>.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("service")]
	public Elastic.Clients.Elasticsearch.Inference.HuggingFaceServiceType Service { get; set; }

	/// <summary>
	/// <para>
	/// Settings used to install the inference model. These settings are specific to the <c>hugging_face</c> service.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("service_settings")]
	public Elastic.Clients.Elasticsearch.Inference.HuggingFaceServiceSettings ServiceSettings { get; set; }
}

/// <summary>
/// <para>
/// Create a Hugging Face inference endpoint.
/// </para>
/// <para>
/// Create an inference endpoint to perform an inference task with the <c>hugging_face</c> service.
/// </para>
/// <para>
/// You must first create an inference endpoint on the Hugging Face endpoint page to get an endpoint URL.
/// Select the model you want to use on the new endpoint creation page (for example <c>intfloat/e5-small-v2</c>), then select the sentence embeddings task under the advanced configuration section.
/// Create the endpoint and copy the URL after the endpoint initialization has been finished.
/// </para>
/// <para>
/// The following models are recommended for the Hugging Face service:
/// </para>
/// <list type="bullet">
/// <item>
/// <para>
/// <c>all-MiniLM-L6-v2</c>
/// </para>
/// </item>
/// <item>
/// <para>
/// <c>all-MiniLM-L12-v2</c>
/// </para>
/// </item>
/// <item>
/// <para>
/// <c>all-mpnet-base-v2</c>
/// </para>
/// </item>
/// <item>
/// <para>
/// <c>e5-base-v2</c>
/// </para>
/// </item>
/// <item>
/// <para>
/// <c>e5-small-v2</c>
/// </para>
/// </item>
/// <item>
/// <para>
/// <c>multilingual-e5-base</c>
/// </para>
/// </item>
/// <item>
/// <para>
/// <c>multilingual-e5-small</c>
/// </para>
/// </item>
/// </list>
/// <para>
/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
/// After creating the endpoint, wait for the model deployment to complete before using it.
/// To verify the deployment status, use the get trained model statistics API.
/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
/// </para>
/// </summary>
public sealed partial class PutHuggingFaceRequestDescriptor : RequestDescriptor<PutHuggingFaceRequestDescriptor, PutHuggingFaceRequestParameters>
{
	internal PutHuggingFaceRequestDescriptor(Action<PutHuggingFaceRequestDescriptor> configure) => configure.Invoke(this);

	public PutHuggingFaceRequestDescriptor(Elastic.Clients.Elasticsearch.Inference.HuggingFaceTaskType taskType, Elastic.Clients.Elasticsearch.Id huggingfaceInferenceId) : base(r => r.Required("task_type", taskType).Required("huggingface_inference_id", huggingfaceInferenceId))
	{
	}

	internal override ApiUrls ApiUrls => ApiUrlLookup.InferencePutHuggingFace;

	protected override HttpMethod StaticHttpMethod => HttpMethod.PUT;

	internal override bool SupportsBody => true;

	internal override string OperationName => "inference.put_hugging_face";

	public PutHuggingFaceRequestDescriptor HuggingfaceInferenceId(Elastic.Clients.Elasticsearch.Id huggingfaceInferenceId)
	{
		RouteValues.Required("huggingface_inference_id", huggingfaceInferenceId);
		return Self;
	}

	public PutHuggingFaceRequestDescriptor TaskType(Elastic.Clients.Elasticsearch.Inference.HuggingFaceTaskType taskType)
	{
		RouteValues.Required("task_type", taskType);
		return Self;
	}

	private Elastic.Clients.Elasticsearch.Inference.InferenceChunkingSettings? ChunkingSettingsValue { get; set; }
	private Elastic.Clients.Elasticsearch.Inference.InferenceChunkingSettingsDescriptor ChunkingSettingsDescriptor { get; set; }
	private Action<Elastic.Clients.Elasticsearch.Inference.InferenceChunkingSettingsDescriptor> ChunkingSettingsDescriptorAction { get; set; }
	private Elastic.Clients.Elasticsearch.Inference.HuggingFaceServiceType ServiceValue { get; set; }
	private Elastic.Clients.Elasticsearch.Inference.HuggingFaceServiceSettings ServiceSettingsValue { get; set; }
	private Elastic.Clients.Elasticsearch.Inference.HuggingFaceServiceSettingsDescriptor ServiceSettingsDescriptor { get; set; }
	private Action<Elastic.Clients.Elasticsearch.Inference.HuggingFaceServiceSettingsDescriptor> ServiceSettingsDescriptorAction { get; set; }

	/// <summary>
	/// <para>
	/// The chunking configuration object.
	/// </para>
	/// </summary>
	public PutHuggingFaceRequestDescriptor ChunkingSettings(Elastic.Clients.Elasticsearch.Inference.InferenceChunkingSettings? chunkingSettings)
	{
		ChunkingSettingsDescriptor = null;
		ChunkingSettingsDescriptorAction = null;
		ChunkingSettingsValue = chunkingSettings;
		return Self;
	}

	public PutHuggingFaceRequestDescriptor ChunkingSettings(Elastic.Clients.Elasticsearch.Inference.InferenceChunkingSettingsDescriptor descriptor)
	{
		ChunkingSettingsValue = null;
		ChunkingSettingsDescriptorAction = null;
		ChunkingSettingsDescriptor = descriptor;
		return Self;
	}

	public PutHuggingFaceRequestDescriptor ChunkingSettings(Action<Elastic.Clients.Elasticsearch.Inference.InferenceChunkingSettingsDescriptor> configure)
	{
		ChunkingSettingsValue = null;
		ChunkingSettingsDescriptor = null;
		ChunkingSettingsDescriptorAction = configure;
		return Self;
	}

	/// <summary>
	/// <para>
	/// The type of service supported for the specified task type. In this case, <c>hugging_face</c>.
	/// </para>
	/// </summary>
	public PutHuggingFaceRequestDescriptor Service(Elastic.Clients.Elasticsearch.Inference.HuggingFaceServiceType service)
	{
		ServiceValue = service;
		return Self;
	}

	/// <summary>
	/// <para>
	/// Settings used to install the inference model. These settings are specific to the <c>hugging_face</c> service.
	/// </para>
	/// </summary>
	public PutHuggingFaceRequestDescriptor ServiceSettings(Elastic.Clients.Elasticsearch.Inference.HuggingFaceServiceSettings serviceSettings)
	{
		ServiceSettingsDescriptor = null;
		ServiceSettingsDescriptorAction = null;
		ServiceSettingsValue = serviceSettings;
		return Self;
	}

	public PutHuggingFaceRequestDescriptor ServiceSettings(Elastic.Clients.Elasticsearch.Inference.HuggingFaceServiceSettingsDescriptor descriptor)
	{
		ServiceSettingsValue = null;
		ServiceSettingsDescriptorAction = null;
		ServiceSettingsDescriptor = descriptor;
		return Self;
	}

	public PutHuggingFaceRequestDescriptor ServiceSettings(Action<Elastic.Clients.Elasticsearch.Inference.HuggingFaceServiceSettingsDescriptor> configure)
	{
		ServiceSettingsValue = null;
		ServiceSettingsDescriptor = null;
		ServiceSettingsDescriptorAction = configure;
		return Self;
	}

	protected override void Serialize(Utf8JsonWriter writer, JsonSerializerOptions options, IElasticsearchClientSettings settings)
	{
		writer.WriteStartObject();
		if (ChunkingSettingsDescriptor is not null)
		{
			writer.WritePropertyName("chunking_settings");
			JsonSerializer.Serialize(writer, ChunkingSettingsDescriptor, options);
		}
		else if (ChunkingSettingsDescriptorAction is not null)
		{
			writer.WritePropertyName("chunking_settings");
			JsonSerializer.Serialize(writer, new Elastic.Clients.Elasticsearch.Inference.InferenceChunkingSettingsDescriptor(ChunkingSettingsDescriptorAction), options);
		}
		else if (ChunkingSettingsValue is not null)
		{
			writer.WritePropertyName("chunking_settings");
			JsonSerializer.Serialize(writer, ChunkingSettingsValue, options);
		}

		writer.WritePropertyName("service");
		JsonSerializer.Serialize(writer, ServiceValue, options);
		if (ServiceSettingsDescriptor is not null)
		{
			writer.WritePropertyName("service_settings");
			JsonSerializer.Serialize(writer, ServiceSettingsDescriptor, options);
		}
		else if (ServiceSettingsDescriptorAction is not null)
		{
			writer.WritePropertyName("service_settings");
			JsonSerializer.Serialize(writer, new Elastic.Clients.Elasticsearch.Inference.HuggingFaceServiceSettingsDescriptor(ServiceSettingsDescriptorAction), options);
		}
		else
		{
			writer.WritePropertyName("service_settings");
			JsonSerializer.Serialize(writer, ServiceSettingsValue, options);
		}

		writer.WriteEndObject();
	}
}