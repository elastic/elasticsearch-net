// Licensed to Elasticsearch B.V under one or more agreements.
// Elasticsearch B.V licenses this file to you under the Apache 2.0 License.
// See the LICENSE file in the project root for more information.
//
// ███╗   ██╗ ██████╗ ████████╗██╗ ██████╗███████╗
// ████╗  ██║██╔═══██╗╚══██╔══╝██║██╔════╝██╔════╝
// ██╔██╗ ██║██║   ██║   ██║   ██║██║     █████╗
// ██║╚██╗██║██║   ██║   ██║   ██║██║     ██╔══╝
// ██║ ╚████║╚██████╔╝   ██║   ██║╚██████╗███████╗
// ╚═╝  ╚═══╝ ╚═════╝    ╚═╝   ╚═╝ ╚═════╝╚══════╝
// ------------------------------------------------
//
// This file is automatically generated.
// Please do not edit these files manually.
//
// ------------------------------------------------

using Elastic.Transport;
using System;
using System.Collections.Generic;
using System.Text.Json;
using System.Text.Json.Serialization;

#nullable restore
namespace Elastic.Clients.Elasticsearch.Analysis
{
	public partial class TokenizerDefinitions : IsADictionaryBase<string, ITokenizerDefinition>
	{
		public TokenizerDefinitions()
		{
		}

		public TokenizerDefinitions(IDictionary<string, ITokenizerDefinition> container) : base(container)
		{
		}

		public void Add(string name, ITokenizerDefinition tokenizerDefinitions) => BackingDictionary.Add(name, tokenizerDefinitions);
	}

	public sealed partial class TokenizerDefinitionsDescriptor : IsADictionaryDescriptor<TokenizerDefinitionsDescriptor, TokenizerDefinitions, string, ITokenizerDefinition>
	{
		public TokenizerDefinitionsDescriptor() : base(new TokenizerDefinitions())
		{
		}

		public TokenizerDefinitionsDescriptor(TokenizerDefinitions tokenizerDefinitions) : base(tokenizerDefinitions ?? new TokenizerDefinitions())
		{
		}

		public TokenizerDefinitionsDescriptor CharGroupTokenizer(string tokenizerDefinitions) => AssignVariant<CharGroupTokenizerDescriptor, CharGroupTokenizer>(tokenizerDefinitions, null);
		public TokenizerDefinitionsDescriptor CharGroupTokenizer(string tokenizerDefinitions, Action<CharGroupTokenizerDescriptor> configure) => AssignVariant<CharGroupTokenizerDescriptor, CharGroupTokenizer>(tokenizerDefinitions, configure);
		public TokenizerDefinitionsDescriptor EdgeNGramTokenizer(string tokenizerDefinitions) => AssignVariant<EdgeNGramTokenizerDescriptor, EdgeNGramTokenizer>(tokenizerDefinitions, null);
		public TokenizerDefinitionsDescriptor EdgeNGramTokenizer(string tokenizerDefinitions, Action<EdgeNGramTokenizerDescriptor> configure) => AssignVariant<EdgeNGramTokenizerDescriptor, EdgeNGramTokenizer>(tokenizerDefinitions, configure);
		public TokenizerDefinitionsDescriptor IcuTokenizer(string tokenizerDefinitions) => AssignVariant<IcuTokenizerDescriptor, IcuTokenizer>(tokenizerDefinitions, null);
		public TokenizerDefinitionsDescriptor IcuTokenizer(string tokenizerDefinitions, Action<IcuTokenizerDescriptor> configure) => AssignVariant<IcuTokenizerDescriptor, IcuTokenizer>(tokenizerDefinitions, configure);
		public TokenizerDefinitionsDescriptor KeywordTokenizer(string tokenizerDefinitions) => AssignVariant<KeywordTokenizerDescriptor, KeywordTokenizer>(tokenizerDefinitions, null);
		public TokenizerDefinitionsDescriptor KeywordTokenizer(string tokenizerDefinitions, Action<KeywordTokenizerDescriptor> configure) => AssignVariant<KeywordTokenizerDescriptor, KeywordTokenizer>(tokenizerDefinitions, configure);
		public TokenizerDefinitionsDescriptor KuromojiTokenizer(string tokenizerDefinitions) => AssignVariant<KuromojiTokenizerDescriptor, KuromojiTokenizer>(tokenizerDefinitions, null);
		public TokenizerDefinitionsDescriptor KuromojiTokenizer(string tokenizerDefinitions, Action<KuromojiTokenizerDescriptor> configure) => AssignVariant<KuromojiTokenizerDescriptor, KuromojiTokenizer>(tokenizerDefinitions, configure);
		public TokenizerDefinitionsDescriptor LetterTokenizer(string tokenizerDefinitions) => AssignVariant<LetterTokenizerDescriptor, LetterTokenizer>(tokenizerDefinitions, null);
		public TokenizerDefinitionsDescriptor LetterTokenizer(string tokenizerDefinitions, Action<LetterTokenizerDescriptor> configure) => AssignVariant<LetterTokenizerDescriptor, LetterTokenizer>(tokenizerDefinitions, configure);
		public TokenizerDefinitionsDescriptor LowercaseTokenizer(string tokenizerDefinitions) => AssignVariant<LowercaseTokenizerDescriptor, LowercaseTokenizer>(tokenizerDefinitions, null);
		public TokenizerDefinitionsDescriptor LowercaseTokenizer(string tokenizerDefinitions, Action<LowercaseTokenizerDescriptor> configure) => AssignVariant<LowercaseTokenizerDescriptor, LowercaseTokenizer>(tokenizerDefinitions, configure);
		public TokenizerDefinitionsDescriptor NGramTokenizer(string tokenizerDefinitions) => AssignVariant<NGramTokenizerDescriptor, NGramTokenizer>(tokenizerDefinitions, null);
		public TokenizerDefinitionsDescriptor NGramTokenizer(string tokenizerDefinitions, Action<NGramTokenizerDescriptor> configure) => AssignVariant<NGramTokenizerDescriptor, NGramTokenizer>(tokenizerDefinitions, configure);
		public TokenizerDefinitionsDescriptor NoriTokenizer(string tokenizerDefinitions) => AssignVariant<NoriTokenizerDescriptor, NoriTokenizer>(tokenizerDefinitions, null);
		public TokenizerDefinitionsDescriptor NoriTokenizer(string tokenizerDefinitions, Action<NoriTokenizerDescriptor> configure) => AssignVariant<NoriTokenizerDescriptor, NoriTokenizer>(tokenizerDefinitions, configure);
		public TokenizerDefinitionsDescriptor PathHierarchyTokenizer(string tokenizerDefinitions) => AssignVariant<PathHierarchyTokenizerDescriptor, PathHierarchyTokenizer>(tokenizerDefinitions, null);
		public TokenizerDefinitionsDescriptor PathHierarchyTokenizer(string tokenizerDefinitions, Action<PathHierarchyTokenizerDescriptor> configure) => AssignVariant<PathHierarchyTokenizerDescriptor, PathHierarchyTokenizer>(tokenizerDefinitions, configure);
		public TokenizerDefinitionsDescriptor PatternTokenizer(string tokenizerDefinitions) => AssignVariant<PatternTokenizerDescriptor, PatternTokenizer>(tokenizerDefinitions, null);
		public TokenizerDefinitionsDescriptor PatternTokenizer(string tokenizerDefinitions, Action<PatternTokenizerDescriptor> configure) => AssignVariant<PatternTokenizerDescriptor, PatternTokenizer>(tokenizerDefinitions, configure);
		public TokenizerDefinitionsDescriptor StandardTokenizer(string tokenizerDefinitions) => AssignVariant<StandardTokenizerDescriptor, StandardTokenizer>(tokenizerDefinitions, null);
		public TokenizerDefinitionsDescriptor StandardTokenizer(string tokenizerDefinitions, Action<StandardTokenizerDescriptor> configure) => AssignVariant<StandardTokenizerDescriptor, StandardTokenizer>(tokenizerDefinitions, configure);
		public TokenizerDefinitionsDescriptor UaxEmailUrlTokenizer(string tokenizerDefinitions) => AssignVariant<UaxEmailUrlTokenizerDescriptor, UaxEmailUrlTokenizer>(tokenizerDefinitions, null);
		public TokenizerDefinitionsDescriptor UaxEmailUrlTokenizer(string tokenizerDefinitions, Action<UaxEmailUrlTokenizerDescriptor> configure) => AssignVariant<UaxEmailUrlTokenizerDescriptor, UaxEmailUrlTokenizer>(tokenizerDefinitions, configure);
		public TokenizerDefinitionsDescriptor WhitespaceTokenizer(string tokenizerDefinitions) => AssignVariant<WhitespaceTokenizerDescriptor, WhitespaceTokenizer>(tokenizerDefinitions, null);
		public TokenizerDefinitionsDescriptor WhitespaceTokenizer(string tokenizerDefinitions, Action<WhitespaceTokenizerDescriptor> configure) => AssignVariant<WhitespaceTokenizerDescriptor, WhitespaceTokenizer>(tokenizerDefinitions, configure);
	}

	internal sealed partial class TokenizerDefinitionInterfaceConverter
	{
		private static ITokenizerDefinition DeserializeVariant(string type, ref Utf8JsonReader reader, JsonSerializerOptions options)
		{
			switch (type)
			{
				case "icu_tokenizer":
					return JsonSerializer.Deserialize<IcuTokenizer>(ref reader, options);
				case "pattern":
					return JsonSerializer.Deserialize<PatternTokenizer>(ref reader, options);
				case "kuromoji_tokenizer":
					return JsonSerializer.Deserialize<KuromojiTokenizer>(ref reader, options);
				case "whitespace":
					return JsonSerializer.Deserialize<WhitespaceTokenizer>(ref reader, options);
				case "uax_url_email":
					return JsonSerializer.Deserialize<UaxEmailUrlTokenizer>(ref reader, options);
				case "standard":
					return JsonSerializer.Deserialize<StandardTokenizer>(ref reader, options);
				case "path_hierarchy":
					return JsonSerializer.Deserialize<PathHierarchyTokenizer>(ref reader, options);
				case "nori_tokenizer":
					return JsonSerializer.Deserialize<NoriTokenizer>(ref reader, options);
				case "ngram":
					return JsonSerializer.Deserialize<NGramTokenizer>(ref reader, options);
				case "lowercase":
					return JsonSerializer.Deserialize<LowercaseTokenizer>(ref reader, options);
				case "letter":
					return JsonSerializer.Deserialize<LetterTokenizer>(ref reader, options);
				case "keyword":
					return JsonSerializer.Deserialize<KeywordTokenizer>(ref reader, options);
				case "edge_ngram":
					return JsonSerializer.Deserialize<EdgeNGramTokenizer>(ref reader, options);
				case "char_group":
					return JsonSerializer.Deserialize<CharGroupTokenizer>(ref reader, options);
				default:
					throw new JsonException("Encounted an unknown variant type which could not be deserialised.");
			}
		}
	}

	public partial interface ITokenizerDefinition
	{
		public string Type { get; }
	}
}