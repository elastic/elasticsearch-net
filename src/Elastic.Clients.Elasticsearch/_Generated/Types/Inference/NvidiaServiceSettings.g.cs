// Licensed to Elasticsearch B.V under one or more agreements.
// Elasticsearch B.V licenses this file to you under the Apache 2.0 License.
// See the LICENSE file in the project root for more information.
//
// ███╗   ██╗ ██████╗ ████████╗██╗ ██████╗███████╗
// ████╗  ██║██╔═══██╗╚══██╔══╝██║██╔════╝██╔════╝
// ██╔██╗ ██║██║   ██║   ██║   ██║██║     █████╗
// ██║╚██╗██║██║   ██║   ██║   ██║██║     ██╔══╝
// ██║ ╚████║╚██████╔╝   ██║   ██║╚██████╗███████╗
// ╚═╝  ╚═══╝ ╚═════╝    ╚═╝   ╚═╝ ╚═════╝╚══════╝
// ------------------------------------------------
//
// This file is automatically generated.
// Please do not edit these files manually.
//
// ------------------------------------------------

#nullable restore

using System;
using System.Linq;
using Elastic.Clients.Elasticsearch.Serialization;

namespace Elastic.Clients.Elasticsearch.Inference;

[System.Text.Json.Serialization.JsonConverter(typeof(Elastic.Clients.Elasticsearch.Inference.Json.NvidiaServiceSettingsConverter))]
public sealed partial class NvidiaServiceSettings
{
	[System.Diagnostics.CodeAnalysis.SetsRequiredMembers]
	public NvidiaServiceSettings(string apiKey, string modelId)
	{
		ApiKey = apiKey;
		ModelId = modelId;
	}

	public NvidiaServiceSettings()
	{
	}

	[System.Diagnostics.CodeAnalysis.SetsRequiredMembers]
	internal NvidiaServiceSettings(Elastic.Clients.Elasticsearch.Serialization.JsonConstructorSentinel sentinel)
	{
		_ = sentinel;
	}

	/// <summary>
	/// <para>
	/// A valid API key for your Nvidia endpoint.
	/// Can be found in <c>API Keys</c> section of Nvidia account settings.
	/// </para>
	/// </summary>
	public required string ApiKey { get; set; }

	/// <summary>
	/// <para>
	/// For a <c>text_embedding</c> task, the maximum number of tokens per input. Inputs exceeding this value are truncated prior to sending to the Nvidia API.
	/// </para>
	/// </summary>
	public int? MaxInputTokens { get; set; }

	/// <summary>
	/// <para>
	/// The name of the model to use for the inference task.
	/// Refer to the model's documentation for the name if needed.
	/// Service has been tested and confirmed to be working with the following models:
	/// </para>
	/// <list type="bullet">
	/// <item>
	/// <para>
	/// For <c>text_embedding</c> task - <c>nvidia/llama-3.2-nv-embedqa-1b-v2</c>.
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// For <c>completion</c> and <c>chat_completion</c> tasks - <c>microsoft/phi-3-mini-128k-instruct</c>.
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// For <c>rerank</c> task - <c>nv-rerank-qa-mistral-4b:1</c>.
	/// Service doesn't support <c>text_embedding</c> task <c>baai/bge-m3</c> and <c>nvidia/nvclip</c> models due to them not recognizing the <c>input_type</c> parameter.
	/// </para>
	/// </item>
	/// </list>
	/// </summary>
	public required string ModelId { get; set; }

	/// <summary>
	/// <para>
	/// This setting helps to minimize the number of rate limit errors returned from the Nvidia API.
	/// By default, the <c>nvidia</c> service sets the number of requests allowed per minute to 3000.
	/// </para>
	/// </summary>
	public Elastic.Clients.Elasticsearch.Inference.RateLimitSetting? RateLimit { get; set; }

	/// <summary>
	/// <para>
	/// For a <c>text_embedding</c> task, the similarity measure. One of cosine, dot_product, l2_norm.
	/// </para>
	/// </summary>
	public Elastic.Clients.Elasticsearch.Inference.NvidiaSimilarityType? Similarity { get; set; }

	/// <summary>
	/// <para>
	/// The URL of the Nvidia model endpoint. If not provided, the default endpoint URL is used depending on the task type:
	/// </para>
	/// <list type="bullet">
	/// <item>
	/// <para>
	/// For <c>text_embedding</c> task - <c>https://integrate.api.nvidia.com/v1/embeddings</c>.
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// For <c>completion</c> and <c>chat_completion</c> tasks - <c>https://integrate.api.nvidia.com/v1/chat/completions</c>.
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// For <c>rerank</c> task - <c>https://ai.api.nvidia.com/v1/retrieval/nvidia/reranking</c>.
	/// </para>
	/// </item>
	/// </list>
	/// </summary>
	public string? Url { get; set; }
}

public readonly partial struct NvidiaServiceSettingsDescriptor
{
	internal Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettings Instance { get; init; }

	[System.Diagnostics.CodeAnalysis.SetsRequiredMembers]
	public NvidiaServiceSettingsDescriptor(Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettings instance)
	{
		Instance = instance;
	}

	[System.Diagnostics.CodeAnalysis.SetsRequiredMembers]
	public NvidiaServiceSettingsDescriptor()
	{
		Instance = new Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettings(Elastic.Clients.Elasticsearch.Serialization.JsonConstructorSentinel.Instance);
	}

	public static explicit operator Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettingsDescriptor(Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettings instance) => new Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettingsDescriptor(instance);
	public static implicit operator Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettings(Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettingsDescriptor descriptor) => descriptor.Instance;

	/// <summary>
	/// <para>
	/// A valid API key for your Nvidia endpoint.
	/// Can be found in <c>API Keys</c> section of Nvidia account settings.
	/// </para>
	/// </summary>
	public Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettingsDescriptor ApiKey(string value)
	{
		Instance.ApiKey = value;
		return this;
	}

	/// <summary>
	/// <para>
	/// For a <c>text_embedding</c> task, the maximum number of tokens per input. Inputs exceeding this value are truncated prior to sending to the Nvidia API.
	/// </para>
	/// </summary>
	public Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettingsDescriptor MaxInputTokens(int? value)
	{
		Instance.MaxInputTokens = value;
		return this;
	}

	/// <summary>
	/// <para>
	/// The name of the model to use for the inference task.
	/// Refer to the model's documentation for the name if needed.
	/// Service has been tested and confirmed to be working with the following models:
	/// </para>
	/// <list type="bullet">
	/// <item>
	/// <para>
	/// For <c>text_embedding</c> task - <c>nvidia/llama-3.2-nv-embedqa-1b-v2</c>.
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// For <c>completion</c> and <c>chat_completion</c> tasks - <c>microsoft/phi-3-mini-128k-instruct</c>.
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// For <c>rerank</c> task - <c>nv-rerank-qa-mistral-4b:1</c>.
	/// Service doesn't support <c>text_embedding</c> task <c>baai/bge-m3</c> and <c>nvidia/nvclip</c> models due to them not recognizing the <c>input_type</c> parameter.
	/// </para>
	/// </item>
	/// </list>
	/// </summary>
	public Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettingsDescriptor ModelId(string value)
	{
		Instance.ModelId = value;
		return this;
	}

	/// <summary>
	/// <para>
	/// This setting helps to minimize the number of rate limit errors returned from the Nvidia API.
	/// By default, the <c>nvidia</c> service sets the number of requests allowed per minute to 3000.
	/// </para>
	/// </summary>
	public Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettingsDescriptor RateLimit(Elastic.Clients.Elasticsearch.Inference.RateLimitSetting? value)
	{
		Instance.RateLimit = value;
		return this;
	}

	/// <summary>
	/// <para>
	/// This setting helps to minimize the number of rate limit errors returned from the Nvidia API.
	/// By default, the <c>nvidia</c> service sets the number of requests allowed per minute to 3000.
	/// </para>
	/// </summary>
	public Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettingsDescriptor RateLimit()
	{
		Instance.RateLimit = Elastic.Clients.Elasticsearch.Inference.RateLimitSettingDescriptor.Build(null);
		return this;
	}

	/// <summary>
	/// <para>
	/// This setting helps to minimize the number of rate limit errors returned from the Nvidia API.
	/// By default, the <c>nvidia</c> service sets the number of requests allowed per minute to 3000.
	/// </para>
	/// </summary>
	public Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettingsDescriptor RateLimit(System.Action<Elastic.Clients.Elasticsearch.Inference.RateLimitSettingDescriptor>? action)
	{
		Instance.RateLimit = Elastic.Clients.Elasticsearch.Inference.RateLimitSettingDescriptor.Build(action);
		return this;
	}

	/// <summary>
	/// <para>
	/// For a <c>text_embedding</c> task, the similarity measure. One of cosine, dot_product, l2_norm.
	/// </para>
	/// </summary>
	public Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettingsDescriptor Similarity(Elastic.Clients.Elasticsearch.Inference.NvidiaSimilarityType? value)
	{
		Instance.Similarity = value;
		return this;
	}

	/// <summary>
	/// <para>
	/// The URL of the Nvidia model endpoint. If not provided, the default endpoint URL is used depending on the task type:
	/// </para>
	/// <list type="bullet">
	/// <item>
	/// <para>
	/// For <c>text_embedding</c> task - <c>https://integrate.api.nvidia.com/v1/embeddings</c>.
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// For <c>completion</c> and <c>chat_completion</c> tasks - <c>https://integrate.api.nvidia.com/v1/chat/completions</c>.
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// For <c>rerank</c> task - <c>https://ai.api.nvidia.com/v1/retrieval/nvidia/reranking</c>.
	/// </para>
	/// </item>
	/// </list>
	/// </summary>
	public Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettingsDescriptor Url(string? value)
	{
		Instance.Url = value;
		return this;
	}

	[System.Runtime.CompilerServices.MethodImpl(System.Runtime.CompilerServices.MethodImplOptions.AggressiveInlining)]
	internal static Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettings Build(System.Action<Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettingsDescriptor> action)
	{
		var builder = new Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettingsDescriptor(new Elastic.Clients.Elasticsearch.Inference.NvidiaServiceSettings(Elastic.Clients.Elasticsearch.Serialization.JsonConstructorSentinel.Instance));
		action.Invoke(builder);
		return builder.Instance;
	}
}