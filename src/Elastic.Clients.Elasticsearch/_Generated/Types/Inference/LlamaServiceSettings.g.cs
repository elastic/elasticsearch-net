// Licensed to Elasticsearch B.V under one or more agreements.
// Elasticsearch B.V licenses this file to you under the Apache 2.0 License.
// See the LICENSE file in the project root for more information.
//
// ███╗   ██╗ ██████╗ ████████╗██╗ ██████╗███████╗
// ████╗  ██║██╔═══██╗╚══██╔══╝██║██╔════╝██╔════╝
// ██╔██╗ ██║██║   ██║   ██║   ██║██║     █████╗
// ██║╚██╗██║██║   ██║   ██║   ██║██║     ██╔══╝
// ██║ ╚████║╚██████╔╝   ██║   ██║╚██████╗███████╗
// ╚═╝  ╚═══╝ ╚═════╝    ╚═╝   ╚═╝ ╚═════╝╚══════╝
// ------------------------------------------------
//
// This file is automatically generated.
// Please do not edit these files manually.
//
// ------------------------------------------------

#nullable restore

using System;
using System.Linq;
using Elastic.Clients.Elasticsearch.Serialization;

namespace Elastic.Clients.Elasticsearch.Inference;

internal sealed partial class LlamaServiceSettingsConverter : System.Text.Json.Serialization.JsonConverter<Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettings>
{
	private static readonly System.Text.Json.JsonEncodedText PropMaxInputTokens = System.Text.Json.JsonEncodedText.Encode("max_input_tokens");
	private static readonly System.Text.Json.JsonEncodedText PropModelId = System.Text.Json.JsonEncodedText.Encode("model_id");
	private static readonly System.Text.Json.JsonEncodedText PropRateLimit = System.Text.Json.JsonEncodedText.Encode("rate_limit");
	private static readonly System.Text.Json.JsonEncodedText PropSimilarity = System.Text.Json.JsonEncodedText.Encode("similarity");
	private static readonly System.Text.Json.JsonEncodedText PropUrl = System.Text.Json.JsonEncodedText.Encode("url");

	public override Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettings Read(ref System.Text.Json.Utf8JsonReader reader, System.Type typeToConvert, System.Text.Json.JsonSerializerOptions options)
	{
		reader.ValidateToken(System.Text.Json.JsonTokenType.StartObject);
		LocalJsonValue<int?> propMaxInputTokens = default;
		LocalJsonValue<string> propModelId = default;
		LocalJsonValue<Elastic.Clients.Elasticsearch.Inference.RateLimitSetting?> propRateLimit = default;
		LocalJsonValue<Elastic.Clients.Elasticsearch.Inference.LlamaSimilarityType?> propSimilarity = default;
		LocalJsonValue<string> propUrl = default;
		while (reader.Read() && reader.TokenType is System.Text.Json.JsonTokenType.PropertyName)
		{
			if (propMaxInputTokens.TryReadProperty(ref reader, options, PropMaxInputTokens, static int? (ref System.Text.Json.Utf8JsonReader r, System.Text.Json.JsonSerializerOptions o) => r.ReadNullableValue<int>(o)))
			{
				continue;
			}

			if (propModelId.TryReadProperty(ref reader, options, PropModelId, null))
			{
				continue;
			}

			if (propRateLimit.TryReadProperty(ref reader, options, PropRateLimit, null))
			{
				continue;
			}

			if (propSimilarity.TryReadProperty(ref reader, options, PropSimilarity, static Elastic.Clients.Elasticsearch.Inference.LlamaSimilarityType? (ref System.Text.Json.Utf8JsonReader r, System.Text.Json.JsonSerializerOptions o) => r.ReadNullableValue<Elastic.Clients.Elasticsearch.Inference.LlamaSimilarityType>(o)))
			{
				continue;
			}

			if (propUrl.TryReadProperty(ref reader, options, PropUrl, null))
			{
				continue;
			}

			if (options.UnmappedMemberHandling is System.Text.Json.Serialization.JsonUnmappedMemberHandling.Skip)
			{
				reader.Skip();
				continue;
			}

			throw new System.Text.Json.JsonException($"Unknown JSON property '{reader.GetString()}' for type '{typeToConvert.Name}'.");
		}

		reader.ValidateToken(System.Text.Json.JsonTokenType.EndObject);
		return new Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettings(Elastic.Clients.Elasticsearch.Serialization.JsonConstructorSentinel.Instance)
		{
			MaxInputTokens = propMaxInputTokens.Value,
			ModelId = propModelId.Value,
			RateLimit = propRateLimit.Value,
			Similarity = propSimilarity.Value,
			Url = propUrl.Value
		};
	}

	public override void Write(System.Text.Json.Utf8JsonWriter writer, Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettings value, System.Text.Json.JsonSerializerOptions options)
	{
		writer.WriteStartObject();
		writer.WriteProperty(options, PropMaxInputTokens, value.MaxInputTokens, null, static (System.Text.Json.Utf8JsonWriter w, System.Text.Json.JsonSerializerOptions o, int? v) => w.WriteNullableValue<int>(o, v));
		writer.WriteProperty(options, PropModelId, value.ModelId, null, null);
		writer.WriteProperty(options, PropRateLimit, value.RateLimit, null, null);
		writer.WriteProperty(options, PropSimilarity, value.Similarity, null, static (System.Text.Json.Utf8JsonWriter w, System.Text.Json.JsonSerializerOptions o, Elastic.Clients.Elasticsearch.Inference.LlamaSimilarityType? v) => w.WriteNullableValue<Elastic.Clients.Elasticsearch.Inference.LlamaSimilarityType>(o, v));
		writer.WriteProperty(options, PropUrl, value.Url, null, null);
		writer.WriteEndObject();
	}
}

[System.Text.Json.Serialization.JsonConverter(typeof(Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettingsConverter))]
public sealed partial class LlamaServiceSettings
{
	[System.Diagnostics.CodeAnalysis.SetsRequiredMembers]
	public LlamaServiceSettings(string modelId, string url)
	{
		ModelId = modelId;
		Url = url;
	}
#if NET7_0_OR_GREATER
	public LlamaServiceSettings()
	{
	}
#endif
#if !NET7_0_OR_GREATER
	[System.Obsolete("The type contains required properties that must be initialized. Please use an alternative constructor to ensure all required values are properly set.")]
	public LlamaServiceSettings()
	{
	}
#endif
	[System.Diagnostics.CodeAnalysis.SetsRequiredMembers]
	internal LlamaServiceSettings(Elastic.Clients.Elasticsearch.Serialization.JsonConstructorSentinel sentinel)
	{
		_ = sentinel;
	}

	/// <summary>
	/// <para>
	/// For a <c>text_embedding</c> task, the maximum number of tokens per input before chunking occurs.
	/// </para>
	/// </summary>
	public int? MaxInputTokens { get; set; }

	/// <summary>
	/// <para>
	/// The name of the model to use for the inference task.
	/// Refer to the Llama downloading models documentation for different ways of getting a list of available models and downloading them.
	/// Service has been tested and confirmed to be working with the following models:
	/// </para>
	/// <list type="bullet">
	/// <item>
	/// <para>
	/// For <c>text_embedding</c> task - <c>all-MiniLM-L6-v2</c>.
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// For <c>completion</c> and <c>chat_completion</c> tasks - <c>llama3.2:3b</c>.
	/// </para>
	/// </item>
	/// </list>
	/// </summary>
	public
#if NET7_0_OR_GREATER
	required
#endif
	string ModelId { get; set; }

	/// <summary>
	/// <para>
	/// This setting helps to minimize the number of rate limit errors returned from the Llama API.
	/// By default, the <c>llama</c> service sets the number of requests allowed per minute to 3000.
	/// </para>
	/// </summary>
	public Elastic.Clients.Elasticsearch.Inference.RateLimitSetting? RateLimit { get; set; }

	/// <summary>
	/// <para>
	/// For a <c>text_embedding</c> task, the similarity measure. One of cosine, dot_product, l2_norm.
	/// </para>
	/// </summary>
	public Elastic.Clients.Elasticsearch.Inference.LlamaSimilarityType? Similarity { get; set; }

	/// <summary>
	/// <para>
	/// The URL endpoint of the Llama stack endpoint.
	/// URL must contain:
	/// </para>
	/// <list type="bullet">
	/// <item>
	/// <para>
	/// For <c>text_embedding</c> task - <c>/v1/inference/embeddings</c>.
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// For <c>completion</c> and <c>chat_completion</c> tasks - <c>/v1/openai/v1/chat/completions</c>.
	/// </para>
	/// </item>
	/// </list>
	/// </summary>
	public
#if NET7_0_OR_GREATER
	required
#endif
	string Url { get; set; }
}

public readonly partial struct LlamaServiceSettingsDescriptor
{
	internal Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettings Instance { get; init; }

	[System.Diagnostics.CodeAnalysis.SetsRequiredMembers]
	public LlamaServiceSettingsDescriptor(Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettings instance)
	{
		Instance = instance;
	}

	[System.Diagnostics.CodeAnalysis.SetsRequiredMembers]
	public LlamaServiceSettingsDescriptor()
	{
		Instance = new Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettings(Elastic.Clients.Elasticsearch.Serialization.JsonConstructorSentinel.Instance);
	}

	public static explicit operator Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettingsDescriptor(Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettings instance) => new Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettingsDescriptor(instance);
	public static implicit operator Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettings(Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettingsDescriptor descriptor) => descriptor.Instance;

	/// <summary>
	/// <para>
	/// For a <c>text_embedding</c> task, the maximum number of tokens per input before chunking occurs.
	/// </para>
	/// </summary>
	public Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettingsDescriptor MaxInputTokens(int? value)
	{
		Instance.MaxInputTokens = value;
		return this;
	}

	/// <summary>
	/// <para>
	/// The name of the model to use for the inference task.
	/// Refer to the Llama downloading models documentation for different ways of getting a list of available models and downloading them.
	/// Service has been tested and confirmed to be working with the following models:
	/// </para>
	/// <list type="bullet">
	/// <item>
	/// <para>
	/// For <c>text_embedding</c> task - <c>all-MiniLM-L6-v2</c>.
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// For <c>completion</c> and <c>chat_completion</c> tasks - <c>llama3.2:3b</c>.
	/// </para>
	/// </item>
	/// </list>
	/// </summary>
	public Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettingsDescriptor ModelId(string value)
	{
		Instance.ModelId = value;
		return this;
	}

	/// <summary>
	/// <para>
	/// This setting helps to minimize the number of rate limit errors returned from the Llama API.
	/// By default, the <c>llama</c> service sets the number of requests allowed per minute to 3000.
	/// </para>
	/// </summary>
	public Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettingsDescriptor RateLimit(Elastic.Clients.Elasticsearch.Inference.RateLimitSetting? value)
	{
		Instance.RateLimit = value;
		return this;
	}

	/// <summary>
	/// <para>
	/// This setting helps to minimize the number of rate limit errors returned from the Llama API.
	/// By default, the <c>llama</c> service sets the number of requests allowed per minute to 3000.
	/// </para>
	/// </summary>
	public Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettingsDescriptor RateLimit()
	{
		Instance.RateLimit = Elastic.Clients.Elasticsearch.Inference.RateLimitSettingDescriptor.Build(null);
		return this;
	}

	/// <summary>
	/// <para>
	/// This setting helps to minimize the number of rate limit errors returned from the Llama API.
	/// By default, the <c>llama</c> service sets the number of requests allowed per minute to 3000.
	/// </para>
	/// </summary>
	public Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettingsDescriptor RateLimit(System.Action<Elastic.Clients.Elasticsearch.Inference.RateLimitSettingDescriptor>? action)
	{
		Instance.RateLimit = Elastic.Clients.Elasticsearch.Inference.RateLimitSettingDescriptor.Build(action);
		return this;
	}

	/// <summary>
	/// <para>
	/// For a <c>text_embedding</c> task, the similarity measure. One of cosine, dot_product, l2_norm.
	/// </para>
	/// </summary>
	public Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettingsDescriptor Similarity(Elastic.Clients.Elasticsearch.Inference.LlamaSimilarityType? value)
	{
		Instance.Similarity = value;
		return this;
	}

	/// <summary>
	/// <para>
	/// The URL endpoint of the Llama stack endpoint.
	/// URL must contain:
	/// </para>
	/// <list type="bullet">
	/// <item>
	/// <para>
	/// For <c>text_embedding</c> task - <c>/v1/inference/embeddings</c>.
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// For <c>completion</c> and <c>chat_completion</c> tasks - <c>/v1/openai/v1/chat/completions</c>.
	/// </para>
	/// </item>
	/// </list>
	/// </summary>
	public Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettingsDescriptor Url(string value)
	{
		Instance.Url = value;
		return this;
	}

	[System.Runtime.CompilerServices.MethodImpl(System.Runtime.CompilerServices.MethodImplOptions.AggressiveInlining)]
	internal static Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettings Build(System.Action<Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettingsDescriptor> action)
	{
		var builder = new Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettingsDescriptor(new Elastic.Clients.Elasticsearch.Inference.LlamaServiceSettings(Elastic.Clients.Elasticsearch.Serialization.JsonConstructorSentinel.Instance));
		action.Invoke(builder);
		return builder.Instance;
	}
}