// Licensed to Elasticsearch B.V under one or more agreements.
// Elasticsearch B.V licenses this file to you under the Apache 2.0 License.
// See the LICENSE file in the project root for more information.
//
// ███╗   ██╗ ██████╗ ████████╗██╗ ██████╗███████╗
// ████╗  ██║██╔═══██╗╚══██╔══╝██║██╔════╝██╔════╝
// ██╔██╗ ██║██║   ██║   ██║   ██║██║     █████╗
// ██║╚██╗██║██║   ██║   ██║   ██║██║     ██╔══╝
// ██║ ╚████║╚██████╔╝   ██║   ██║╚██████╗███████╗
// ╚═╝  ╚═══╝ ╚═════╝    ╚═╝   ╚═╝ ╚═════╝╚══════╝
// ------------------------------------------------
//
// This file is automatically generated.
// Please do not edit these files manually.
//
// ------------------------------------------------

#nullable restore

using Elastic.Clients.Elasticsearch.Fluent;
using Elastic.Clients.Elasticsearch.Serialization;
using System;
using System.Collections.Generic;
using System.Linq.Expressions;
using System.Text.Json;
using System.Text.Json.Serialization;

namespace Elastic.Clients.Elasticsearch.Inference;

public sealed partial class AzureAiStudioServiceSettings
{
	/// <summary>
	/// <para>
	/// A valid API key of your Azure AI Studio model deployment.
	/// This key can be found on the overview page for your deployment in the management section of your Azure AI Studio account.
	/// </para>
	/// <para>
	/// IMPORTANT: You need to provide the API key only once, during the inference model creation.
	/// The get inference endpoint API does not retrieve your API key.
	/// After creating the inference model, you cannot change the associated API key.
	/// If you want to use a different API key, delete the inference model and recreate it with the same name and the updated API key.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("api_key")]
	public string ApiKey { get; set; }

	/// <summary>
	/// <para>
	/// The type of endpoint that is available for deployment through Azure AI Studio: <c>token</c> or <c>realtime</c>.
	/// The <c>token</c> endpoint type is for "pay as you go" endpoints that are billed per token.
	/// The <c>realtime</c> endpoint type is for "real-time" endpoints that are billed per hour of usage.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("endpoint_type")]
	public string EndpointType { get; set; }

	/// <summary>
	/// <para>
	/// The model provider for your deployment.
	/// Note that some providers may support only certain task types.
	/// Supported providers include:
	/// </para>
	/// <list type="bullet">
	/// <item>
	/// <para>
	/// <c>cohere</c> - available for <c>text_embedding</c> and <c>completion</c> task types
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// <c>databricks</c> - available for <c>completion</c> task type only
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// <c>meta</c> - available for <c>completion</c> task type only
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// <c>microsoft_phi</c> - available for <c>completion</c> task type only
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// <c>mistral</c> - available for <c>completion</c> task type only
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// <c>openai</c> - available for <c>text_embedding</c> and <c>completion</c> task types
	/// </para>
	/// </item>
	/// </list>
	/// </summary>
	[JsonInclude, JsonPropertyName("provider")]
	public string Provider { get; set; }

	/// <summary>
	/// <para>
	/// This setting helps to minimize the number of rate limit errors returned from Azure AI Studio.
	/// By default, the <c>azureaistudio</c> service sets the number of requests allowed per minute to 240.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("rate_limit")]
	public Elastic.Clients.Elasticsearch.Inference.RateLimitSetting? RateLimit { get; set; }

	/// <summary>
	/// <para>
	/// The target URL of your Azure AI Studio model deployment.
	/// This can be found on the overview page for your deployment in the management section of your Azure AI Studio account.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("target")]
	public string Target { get; set; }
}

public sealed partial class AzureAiStudioServiceSettingsDescriptor : SerializableDescriptor<AzureAiStudioServiceSettingsDescriptor>
{
	internal AzureAiStudioServiceSettingsDescriptor(Action<AzureAiStudioServiceSettingsDescriptor> configure) => configure.Invoke(this);

	public AzureAiStudioServiceSettingsDescriptor() : base()
	{
	}

	private string ApiKeyValue { get; set; }
	private string EndpointTypeValue { get; set; }
	private string ProviderValue { get; set; }
	private Elastic.Clients.Elasticsearch.Inference.RateLimitSetting? RateLimitValue { get; set; }
	private Elastic.Clients.Elasticsearch.Inference.RateLimitSettingDescriptor RateLimitDescriptor { get; set; }
	private Action<Elastic.Clients.Elasticsearch.Inference.RateLimitSettingDescriptor> RateLimitDescriptorAction { get; set; }
	private string TargetValue { get; set; }

	/// <summary>
	/// <para>
	/// A valid API key of your Azure AI Studio model deployment.
	/// This key can be found on the overview page for your deployment in the management section of your Azure AI Studio account.
	/// </para>
	/// <para>
	/// IMPORTANT: You need to provide the API key only once, during the inference model creation.
	/// The get inference endpoint API does not retrieve your API key.
	/// After creating the inference model, you cannot change the associated API key.
	/// If you want to use a different API key, delete the inference model and recreate it with the same name and the updated API key.
	/// </para>
	/// </summary>
	public AzureAiStudioServiceSettingsDescriptor ApiKey(string apiKey)
	{
		ApiKeyValue = apiKey;
		return Self;
	}

	/// <summary>
	/// <para>
	/// The type of endpoint that is available for deployment through Azure AI Studio: <c>token</c> or <c>realtime</c>.
	/// The <c>token</c> endpoint type is for "pay as you go" endpoints that are billed per token.
	/// The <c>realtime</c> endpoint type is for "real-time" endpoints that are billed per hour of usage.
	/// </para>
	/// </summary>
	public AzureAiStudioServiceSettingsDescriptor EndpointType(string endpointType)
	{
		EndpointTypeValue = endpointType;
		return Self;
	}

	/// <summary>
	/// <para>
	/// The model provider for your deployment.
	/// Note that some providers may support only certain task types.
	/// Supported providers include:
	/// </para>
	/// <list type="bullet">
	/// <item>
	/// <para>
	/// <c>cohere</c> - available for <c>text_embedding</c> and <c>completion</c> task types
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// <c>databricks</c> - available for <c>completion</c> task type only
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// <c>meta</c> - available for <c>completion</c> task type only
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// <c>microsoft_phi</c> - available for <c>completion</c> task type only
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// <c>mistral</c> - available for <c>completion</c> task type only
	/// </para>
	/// </item>
	/// <item>
	/// <para>
	/// <c>openai</c> - available for <c>text_embedding</c> and <c>completion</c> task types
	/// </para>
	/// </item>
	/// </list>
	/// </summary>
	public AzureAiStudioServiceSettingsDescriptor Provider(string provider)
	{
		ProviderValue = provider;
		return Self;
	}

	/// <summary>
	/// <para>
	/// This setting helps to minimize the number of rate limit errors returned from Azure AI Studio.
	/// By default, the <c>azureaistudio</c> service sets the number of requests allowed per minute to 240.
	/// </para>
	/// </summary>
	public AzureAiStudioServiceSettingsDescriptor RateLimit(Elastic.Clients.Elasticsearch.Inference.RateLimitSetting? rateLimit)
	{
		RateLimitDescriptor = null;
		RateLimitDescriptorAction = null;
		RateLimitValue = rateLimit;
		return Self;
	}

	public AzureAiStudioServiceSettingsDescriptor RateLimit(Elastic.Clients.Elasticsearch.Inference.RateLimitSettingDescriptor descriptor)
	{
		RateLimitValue = null;
		RateLimitDescriptorAction = null;
		RateLimitDescriptor = descriptor;
		return Self;
	}

	public AzureAiStudioServiceSettingsDescriptor RateLimit(Action<Elastic.Clients.Elasticsearch.Inference.RateLimitSettingDescriptor> configure)
	{
		RateLimitValue = null;
		RateLimitDescriptor = null;
		RateLimitDescriptorAction = configure;
		return Self;
	}

	/// <summary>
	/// <para>
	/// The target URL of your Azure AI Studio model deployment.
	/// This can be found on the overview page for your deployment in the management section of your Azure AI Studio account.
	/// </para>
	/// </summary>
	public AzureAiStudioServiceSettingsDescriptor Target(string target)
	{
		TargetValue = target;
		return Self;
	}

	protected override void Serialize(Utf8JsonWriter writer, JsonSerializerOptions options, IElasticsearchClientSettings settings)
	{
		writer.WriteStartObject();
		writer.WritePropertyName("api_key");
		writer.WriteStringValue(ApiKeyValue);
		writer.WritePropertyName("endpoint_type");
		writer.WriteStringValue(EndpointTypeValue);
		writer.WritePropertyName("provider");
		writer.WriteStringValue(ProviderValue);
		if (RateLimitDescriptor is not null)
		{
			writer.WritePropertyName("rate_limit");
			JsonSerializer.Serialize(writer, RateLimitDescriptor, options);
		}
		else if (RateLimitDescriptorAction is not null)
		{
			writer.WritePropertyName("rate_limit");
			JsonSerializer.Serialize(writer, new Elastic.Clients.Elasticsearch.Inference.RateLimitSettingDescriptor(RateLimitDescriptorAction), options);
		}
		else if (RateLimitValue is not null)
		{
			writer.WritePropertyName("rate_limit");
			JsonSerializer.Serialize(writer, RateLimitValue, options);
		}

		writer.WritePropertyName("target");
		writer.WriteStringValue(TargetValue);
		writer.WriteEndObject();
	}
}