// Licensed to Elasticsearch B.V under one or more agreements.
// Elasticsearch B.V licenses this file to you under the Apache 2.0 License.
// See the LICENSE file in the project root for more information.
//
// ███╗   ██╗ ██████╗ ████████╗██╗ ██████╗███████╗
// ████╗  ██║██╔═══██╗╚══██╔══╝██║██╔════╝██╔════╝
// ██╔██╗ ██║██║   ██║   ██║   ██║██║     █████╗
// ██║╚██╗██║██║   ██║   ██║   ██║██║     ██╔══╝
// ██║ ╚████║╚██████╔╝   ██║   ██║╚██████╗███████╗
// ╚═╝  ╚═══╝ ╚═════╝    ╚═╝   ╚═╝ ╚═════╝╚══════╝
// ------------------------------------------------
//
// This file is automatically generated.
// Please do not edit these files manually.
//
// ------------------------------------------------

#nullable restore

using Elastic.Clients.Elasticsearch.Serverless.Fluent;
using Elastic.Clients.Elasticsearch.Serverless.Serialization;
using System;
using System.Collections.Generic;
using System.Linq.Expressions;
using System.Text.Json;
using System.Text.Json.Serialization;

namespace Elastic.Clients.Elasticsearch.Serverless.MachineLearning;

public sealed partial class TrainedModelDeploymentStats
{
	[JsonInclude, JsonPropertyName("adaptive_allocations")]
	public Elastic.Clients.Elasticsearch.Serverless.MachineLearning.AdaptiveAllocationsSettings? AdaptiveAllocations { get; init; }

	/// <summary>
	/// <para>
	/// The detailed allocation status for the deployment.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("allocation_status")]
	public Elastic.Clients.Elasticsearch.Serverless.MachineLearning.TrainedModelDeploymentAllocationStatus? AllocationStatus { get; init; }
	[JsonInclude, JsonPropertyName("cache_size")]
	public Elastic.Clients.Elasticsearch.Serverless.ByteSize? CacheSize { get; init; }

	/// <summary>
	/// <para>
	/// The unique identifier for the trained model deployment.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("deployment_id")]
	public string DeploymentId { get; init; }

	/// <summary>
	/// <para>
	/// The sum of <c>error_count</c> for all nodes in the deployment.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("error_count")]
	public int? ErrorCount { get; init; }

	/// <summary>
	/// <para>
	/// The sum of <c>inference_count</c> for all nodes in the deployment.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("inference_count")]
	public int? InferenceCount { get; init; }

	/// <summary>
	/// <para>
	/// The unique identifier for the trained model.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("model_id")]
	public string ModelId { get; init; }

	/// <summary>
	/// <para>
	/// The deployment stats for each node that currently has the model allocated.
	/// In serverless, stats are reported for a single unnamed virtual node.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("nodes")]
	public IReadOnlyCollection<Elastic.Clients.Elasticsearch.Serverless.MachineLearning.TrainedModelDeploymentNodesStats> Nodes { get; init; }

	/// <summary>
	/// <para>
	/// The number of allocations requested.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("number_of_allocations")]
	public int? NumberOfAllocations { get; init; }
	[JsonInclude, JsonPropertyName("peak_throughput_per_minute")]
	public long PeakThroughputPerMinute { get; init; }
	[JsonInclude, JsonPropertyName("priority")]
	public Elastic.Clients.Elasticsearch.Serverless.MachineLearning.TrainingPriority Priority { get; init; }

	/// <summary>
	/// <para>
	/// The number of inference requests that can be queued before new requests are rejected.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("queue_capacity")]
	public int? QueueCapacity { get; init; }

	/// <summary>
	/// <para>
	/// The reason for the current deployment state. Usually only populated when
	/// the model is not deployed to a node.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("reason")]
	public string? Reason { get; init; }

	/// <summary>
	/// <para>
	/// The sum of <c>rejected_execution_count</c> for all nodes in the deployment.
	/// Individual nodes reject an inference request if the inference queue is full.
	/// The queue size is controlled by the <c>queue_capacity</c> setting in the start
	/// trained model deployment API.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("rejected_execution_count")]
	public int? RejectedExecutionCount { get; init; }

	/// <summary>
	/// <para>
	/// The epoch timestamp when the deployment started.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("start_time")]
	public long StartTime { get; init; }

	/// <summary>
	/// <para>
	/// The overall state of the deployment.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("state")]
	public Elastic.Clients.Elasticsearch.Serverless.MachineLearning.DeploymentAssignmentState? State { get; init; }

	/// <summary>
	/// <para>
	/// The number of threads used be each allocation during inference.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("threads_per_allocation")]
	public int? ThreadsPerAllocation { get; init; }

	/// <summary>
	/// <para>
	/// The sum of <c>timeout_count</c> for all nodes in the deployment.
	/// </para>
	/// </summary>
	[JsonInclude, JsonPropertyName("timeout_count")]
	public int? TimeoutCount { get; init; }
}