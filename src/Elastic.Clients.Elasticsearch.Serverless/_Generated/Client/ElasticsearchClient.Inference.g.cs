// Licensed to Elasticsearch B.V under one or more agreements.
// Elasticsearch B.V licenses this file to you under the Apache 2.0 License.
// See the LICENSE file in the project root for more information.
//
// ███╗   ██╗ ██████╗ ████████╗██╗ ██████╗███████╗
// ████╗  ██║██╔═══██╗╚══██╔══╝██║██╔════╝██╔════╝
// ██╔██╗ ██║██║   ██║   ██║   ██║██║     █████╗
// ██║╚██╗██║██║   ██║   ██║   ██║██║     ██╔══╝
// ██║ ╚████║╚██████╔╝   ██║   ██║╚██████╗███████╗
// ╚═╝  ╚═══╝ ╚═════╝    ╚═╝   ╚═╝ ╚═════╝╚══════╝
// ------------------------------------------------
//
// This file is automatically generated.
// Please do not edit these files manually.
//
// ------------------------------------------------

#nullable restore

using System;
using System.Collections.Generic;
using System.Threading;
using System.Threading.Tasks;

namespace Elastic.Clients.Elasticsearch.Serverless.Inference;

public partial class InferenceNamespacedClient : NamespacedClientProxy
{
	/// <summary>
	/// <para>
	/// Initializes a new instance of the <see cref="InferenceNamespacedClient"/> class for mocking.
	/// </para>
	/// </summary>
	protected InferenceNamespacedClient() : base()
	{
	}

	internal InferenceNamespacedClient(ElasticsearchClient client) : base(client)
	{
	}

	/// <summary>
	/// <para>
	/// Delete an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<DeleteInferenceResponse> DeleteAsync(DeleteInferenceRequest request, CancellationToken cancellationToken = default)
	{
		request.BeforeRequest();
		return DoRequestAsync<DeleteInferenceRequest, DeleteInferenceResponse, DeleteInferenceRequestParameters>(request, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Delete an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<DeleteInferenceResponse> DeleteAsync(DeleteInferenceRequestDescriptor descriptor, CancellationToken cancellationToken = default)
	{
		descriptor.BeforeRequest();
		return DoRequestAsync<DeleteInferenceRequestDescriptor, DeleteInferenceResponse, DeleteInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Delete an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<DeleteInferenceResponse> DeleteAsync(Elastic.Clients.Elasticsearch.Serverless.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Serverless.Id inferenceId, CancellationToken cancellationToken = default)
	{
		var descriptor = new DeleteInferenceRequestDescriptor(taskType, inferenceId);
		descriptor.BeforeRequest();
		return DoRequestAsync<DeleteInferenceRequestDescriptor, DeleteInferenceResponse, DeleteInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Delete an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<DeleteInferenceResponse> DeleteAsync(Elastic.Clients.Elasticsearch.Serverless.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Serverless.Id inferenceId, Action<DeleteInferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new DeleteInferenceRequestDescriptor(taskType, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<DeleteInferenceRequestDescriptor, DeleteInferenceResponse, DeleteInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Delete an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<DeleteInferenceResponse> DeleteAsync(Elastic.Clients.Elasticsearch.Serverless.Id inferenceId, CancellationToken cancellationToken = default)
	{
		var descriptor = new DeleteInferenceRequestDescriptor(inferenceId);
		descriptor.BeforeRequest();
		return DoRequestAsync<DeleteInferenceRequestDescriptor, DeleteInferenceResponse, DeleteInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Delete an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<DeleteInferenceResponse> DeleteAsync(Elastic.Clients.Elasticsearch.Serverless.Id inferenceId, Action<DeleteInferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new DeleteInferenceRequestDescriptor(inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<DeleteInferenceRequestDescriptor, DeleteInferenceResponse, DeleteInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Get an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/get-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<GetInferenceResponse> GetAsync(GetInferenceRequest request, CancellationToken cancellationToken = default)
	{
		request.BeforeRequest();
		return DoRequestAsync<GetInferenceRequest, GetInferenceResponse, GetInferenceRequestParameters>(request, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Get an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/get-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<GetInferenceResponse> GetAsync(GetInferenceRequestDescriptor descriptor, CancellationToken cancellationToken = default)
	{
		descriptor.BeforeRequest();
		return DoRequestAsync<GetInferenceRequestDescriptor, GetInferenceResponse, GetInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Get an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/get-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<GetInferenceResponse> GetAsync(Elastic.Clients.Elasticsearch.Serverless.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Serverless.Id? inferenceId, CancellationToken cancellationToken = default)
	{
		var descriptor = new GetInferenceRequestDescriptor(taskType, inferenceId);
		descriptor.BeforeRequest();
		return DoRequestAsync<GetInferenceRequestDescriptor, GetInferenceResponse, GetInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Get an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/get-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<GetInferenceResponse> GetAsync(Elastic.Clients.Elasticsearch.Serverless.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Serverless.Id? inferenceId, Action<GetInferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new GetInferenceRequestDescriptor(taskType, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<GetInferenceRequestDescriptor, GetInferenceResponse, GetInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Get an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/get-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<GetInferenceResponse> GetAsync(CancellationToken cancellationToken = default)
	{
		var descriptor = new GetInferenceRequestDescriptor();
		descriptor.BeforeRequest();
		return DoRequestAsync<GetInferenceRequestDescriptor, GetInferenceResponse, GetInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Get an inference endpoint
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/get-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<GetInferenceResponse> GetAsync(Action<GetInferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new GetInferenceRequestDescriptor();
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<GetInferenceRequestDescriptor, GetInferenceResponse, GetInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Perform inference on the service
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/post-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<InferenceResponse> InferenceAsync(InferenceRequest request, CancellationToken cancellationToken = default)
	{
		request.BeforeRequest();
		return DoRequestAsync<InferenceRequest, InferenceResponse, InferenceRequestParameters>(request, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Perform inference on the service
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/post-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<InferenceResponse> InferenceAsync(InferenceRequestDescriptor descriptor, CancellationToken cancellationToken = default)
	{
		descriptor.BeforeRequest();
		return DoRequestAsync<InferenceRequestDescriptor, InferenceResponse, InferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Perform inference on the service
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/post-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<InferenceResponse> InferenceAsync(Elastic.Clients.Elasticsearch.Serverless.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Serverless.Id inferenceId, CancellationToken cancellationToken = default)
	{
		var descriptor = new InferenceRequestDescriptor(taskType, inferenceId);
		descriptor.BeforeRequest();
		return DoRequestAsync<InferenceRequestDescriptor, InferenceResponse, InferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Perform inference on the service
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/post-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<InferenceResponse> InferenceAsync(Elastic.Clients.Elasticsearch.Serverless.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Serverless.Id inferenceId, Action<InferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new InferenceRequestDescriptor(taskType, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<InferenceRequestDescriptor, InferenceResponse, InferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Perform inference on the service
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/post-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<InferenceResponse> InferenceAsync(Elastic.Clients.Elasticsearch.Serverless.Id inferenceId, CancellationToken cancellationToken = default)
	{
		var descriptor = new InferenceRequestDescriptor(inferenceId);
		descriptor.BeforeRequest();
		return DoRequestAsync<InferenceRequestDescriptor, InferenceResponse, InferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Perform inference on the service
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/post-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<InferenceResponse> InferenceAsync(Elastic.Clients.Elasticsearch.Serverless.Id inferenceId, Action<InferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new InferenceRequestDescriptor(inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<InferenceRequestDescriptor, InferenceResponse, InferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Create an inference endpoint.
	/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
	/// After creating the endpoint, wait for the model deployment to complete before using it.
	/// To verify the deployment status, use the get trained model statistics API.
	/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
	/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Mistral, Azure OpenAI, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<PutInferenceResponse> PutAsync(PutInferenceRequest request, CancellationToken cancellationToken = default)
	{
		request.BeforeRequest();
		return DoRequestAsync<PutInferenceRequest, PutInferenceResponse, PutInferenceRequestParameters>(request, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Create an inference endpoint.
	/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
	/// After creating the endpoint, wait for the model deployment to complete before using it.
	/// To verify the deployment status, use the get trained model statistics API.
	/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
	/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Mistral, Azure OpenAI, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<PutInferenceResponse> PutAsync(PutInferenceRequestDescriptor descriptor, CancellationToken cancellationToken = default)
	{
		descriptor.BeforeRequest();
		return DoRequestAsync<PutInferenceRequestDescriptor, PutInferenceResponse, PutInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Create an inference endpoint.
	/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
	/// After creating the endpoint, wait for the model deployment to complete before using it.
	/// To verify the deployment status, use the get trained model statistics API.
	/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
	/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Mistral, Azure OpenAI, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<PutInferenceResponse> PutAsync(Elastic.Clients.Elasticsearch.Serverless.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Serverless.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Serverless.Id inferenceId, CancellationToken cancellationToken = default)
	{
		var descriptor = new PutInferenceRequestDescriptor(inferenceConfig, taskType, inferenceId);
		descriptor.BeforeRequest();
		return DoRequestAsync<PutInferenceRequestDescriptor, PutInferenceResponse, PutInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Create an inference endpoint.
	/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
	/// After creating the endpoint, wait for the model deployment to complete before using it.
	/// To verify the deployment status, use the get trained model statistics API.
	/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
	/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Mistral, Azure OpenAI, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<PutInferenceResponse> PutAsync(Elastic.Clients.Elasticsearch.Serverless.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Serverless.Inference.TaskType? taskType, Elastic.Clients.Elasticsearch.Serverless.Id inferenceId, Action<PutInferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new PutInferenceRequestDescriptor(inferenceConfig, taskType, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<PutInferenceRequestDescriptor, PutInferenceResponse, PutInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Create an inference endpoint.
	/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
	/// After creating the endpoint, wait for the model deployment to complete before using it.
	/// To verify the deployment status, use the get trained model statistics API.
	/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
	/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Mistral, Azure OpenAI, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<PutInferenceResponse> PutAsync(Elastic.Clients.Elasticsearch.Serverless.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Serverless.Id inferenceId, CancellationToken cancellationToken = default)
	{
		var descriptor = new PutInferenceRequestDescriptor(inferenceConfig, inferenceId);
		descriptor.BeforeRequest();
		return DoRequestAsync<PutInferenceRequestDescriptor, PutInferenceResponse, PutInferenceRequestParameters>(descriptor, cancellationToken);
	}

	/// <summary>
	/// <para>
	/// Create an inference endpoint.
	/// When you create an inference endpoint, the associated machine learning model is automatically deployed if it is not already running.
	/// After creating the endpoint, wait for the model deployment to complete before using it.
	/// To verify the deployment status, use the get trained model statistics API.
	/// Look for <c>"state": "fully_allocated"</c> in the response and ensure that the <c>"allocation_count"</c> matches the <c>"target_allocation_count"</c>.
	/// Avoid creating multiple endpoints for the same model unless required, as each endpoint consumes significant resources.
	/// </para>
	/// <para>
	/// IMPORTANT: The inference APIs enable you to use certain services, such as built-in machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Mistral, Azure OpenAI, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.
	/// For built-in models and models uploaded through Eland, the inference APIs offer an alternative way to use and manage trained models.
	/// However, if you do not plan to use the inference APIs to use these models or if you want to use non-NLP models, use the machine learning trained model APIs.
	/// </para>
	/// <para><see href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-inference-api.html">Learn more about this API in the Elasticsearch documentation.</see></para>
	/// </summary>
	public virtual Task<PutInferenceResponse> PutAsync(Elastic.Clients.Elasticsearch.Serverless.Inference.InferenceEndpoint inferenceConfig, Elastic.Clients.Elasticsearch.Serverless.Id inferenceId, Action<PutInferenceRequestDescriptor> configureRequest, CancellationToken cancellationToken = default)
	{
		var descriptor = new PutInferenceRequestDescriptor(inferenceConfig, inferenceId);
		configureRequest?.Invoke(descriptor);
		descriptor.BeforeRequest();
		return DoRequestAsync<PutInferenceRequestDescriptor, PutInferenceResponse, PutInferenceRequestParameters>(descriptor, cancellationToken);
	}
}